{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME: C:\\Program Files\\Java\\jdk-17\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"JAVA_HOME:\", os.environ.get(\"JAVA_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"leetcode_tasks\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recyclable and Low Fat Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [['0', 'Y', 'N'], ['1', 'Y', 'Y'], ['2', 'N', 'Y'], ['3', 'Y', 'Y'], ['4', 'N', 'N']]\n",
    "products = pd.DataFrame(data, columns=['product_id', 'low_fats', 'recyclable']).astype(\n",
    "    {'product_id': 'int64', 'low_fats': 'category', 'recyclable': 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- low_fats: string (nullable = true)\n",
      " |-- recyclable: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df = spark.createDataFrame(products)\n",
    "products_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+\n",
      "|product_id|low_fats|recyclable|\n",
      "+----------+--------+----------+\n",
      "|         0|       Y|         N|\n",
      "|         1|       Y|         Y|\n",
      "|         2|       N|         Y|\n",
      "|         3|       Y|         Y|\n",
      "|         4|       N|         N|\n",
      "+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find the ids of products that are both low fat and recyclable.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|product_id|\n",
      "+----------+\n",
      "|         1|\n",
      "|         3|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.filter((products_df['low_fats'] == 'Y') & (products_df['recyclable'] == 'Y')).select('product_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Customer Referee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'Will', None], [2, 'Jane', None], [3, 'Alex', 2], [4, 'Bill', None], [5, 'Zack', 1], [6, 'Mark', 2]]\n",
    "customer = pd.DataFrame(data, columns=['id', 'name', 'referee_id']).astype(\n",
    "    {'id': 'Int64', 'name': 'object', 'referee_id': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = spark.createDataFrame(customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- referee_id: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+\n",
      "| id|name|referee_id|\n",
      "+---+----+----------+\n",
      "|  1|Will|       NaN|\n",
      "|  2|Jane|       NaN|\n",
      "|  3|Alex|       2.0|\n",
      "|  4|Bill|       NaN|\n",
      "|  5|Zack|       1.0|\n",
      "|  6|Mark|       2.0|\n",
      "+---+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the names of the customer that are not referred by the customer with id = 2.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|Will|\n",
      "|Jane|\n",
      "|Bill|\n",
      "|Zack|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.filter(customer_df['referee_id'] != 2).select('name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['Afghanistan', 'Asia', 652230, 25500100, 20343000000], ['Albania', 'Europe', 28748, 2831741, 12960000000],\n",
    "        ['Algeria', 'Africa', 2381741, 37100000, 188681000000], ['Andorra', 'Europe', 468, 78115, 3712000000],\n",
    "        ['Angola', 'Africa', 1246700, 20609294, 100990000000]]\n",
    "world = pd.DataFrame(data, columns=['name', 'continent', 'area', 'population', 'gdp']).astype(\n",
    "    {'name': 'object', 'continent': 'object', 'area': 'Int64', 'population': 'Int64', 'gdp': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_df = spark.createDataFrame(world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------+----------+------------+\n",
      "|       name|continent|   area|population|         gdp|\n",
      "+-----------+---------+-------+----------+------------+\n",
      "|Afghanistan|     Asia| 652230|  25500100| 20343000000|\n",
      "|    Albania|   Europe|  28748|   2831741| 12960000000|\n",
      "|    Algeria|   Africa|2381741|  37100000|188681000000|\n",
      "|    Andorra|   Europe|    468|     78115|  3712000000|\n",
      "|     Angola|   Africa|1246700|  20609294|100990000000|\n",
      "+-----------+---------+-------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "world_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A country is big if:\n",
    "\n",
    "it has an area of at least three million (i.e., 3000000 km2), or it has a population of at least twenty-five million (i.e., 25000000). Write a solution to find the name, population, and area of the big countries.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+\n",
      "|       name|population|   area|\n",
      "+-----------+----------+-------+\n",
      "|Afghanistan|  25500100| 652230|\n",
      "|    Algeria|  37100000|2381741|\n",
      "+-----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "world_df.filter((world_df['area'] >= 3000000) | (world_df['population'] >= 25000000)).select(['name', 'population', 'area']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Views I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 3, 5, '2019-08-01'], [1, 3, 6, '2019-08-02'], [2, 7, 7, '2019-08-01'], [2, 7, 6, '2019-08-02'], [4, 7, 1, '2019-07-22'], [3, 4, 4, '2019-07-21'], [3, 4, 4, '2019-07-21']]\n",
    "views = pd.DataFrame(data, columns=['article_id', 'author_id', 'viewer_id', 'view_date']).astype({'article_id':'Int64', 'author_id':'Int64', 'viewer_id':'Int64', 'view_date':'datetime64[ns]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_df = spark.createDataFrame(views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+-------------------+\n",
      "|article_id|author_id|viewer_id|          view_date|\n",
      "+----------+---------+---------+-------------------+\n",
      "|         1|        3|        5|2019-08-01 00:00:00|\n",
      "|         1|        3|        6|2019-08-02 00:00:00|\n",
      "|         2|        7|        7|2019-08-01 00:00:00|\n",
      "|         2|        7|        6|2019-08-02 00:00:00|\n",
      "|         4|        7|        1|2019-07-22 00:00:00|\n",
      "|         3|        4|        4|2019-07-21 00:00:00|\n",
      "|         3|        4|        4|2019-07-21 00:00:00|\n",
      "+----------+---------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find all the authors that viewed at least one of their own articles.\n",
    "\n",
    "Return the result table sorted by id in ascending order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  4|\n",
      "|  7|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views_df.filter(views_df['author_id'] == views_df['viewer_id']).select(views_df['author_id'].alias('id')).distinct().orderBy('author_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invalid Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'Vote for Biden'], [2, 'Let us make America great again!']]\n",
    "tweets = pd.DataFrame(data, columns=['tweet_id', 'content']).astype({'tweet_id':'Int64', 'content':'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = spark.createDataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|tweet_id|             content|\n",
      "+--------+--------------------+\n",
      "|       1|      Vote for Biden|\n",
      "|       2|Let us make Ameri...|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find the IDs of the invalid tweets. The tweet is invalid if the number of characters used in the content of the tweet is strictly greater than 15.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|tweet_id|\n",
      "+--------+\n",
      "|       2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "tweets_df.filter(F.length(tweets_df['content']) > 15).select('tweet_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Employee ID With The Unique Identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'Alice'], [7, 'Bob'], [11, 'Meir'], [90, 'Winston'], [3, 'Jonathan']]\n",
    "employees = pd.DataFrame(data, columns=['id', 'name']).astype({'id':'int64', 'name':'object'})\n",
    "data = [[3, 1], [11, 2], [90, 3]]\n",
    "employee_uni = pd.DataFrame(data, columns=['id', 'unique_id']).astype({'id':'int64', 'unique_id':'int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|id |name    |\n",
      "+---+--------+\n",
      "|1  |Alice   |\n",
      "|7  |Bob     |\n",
      "|11 |Meir    |\n",
      "|90 |Winston |\n",
      "|3  |Jonathan|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df = spark.createDataFrame(employees)\n",
    "employees_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|id |unique_id|\n",
      "+---+---------+\n",
      "|3  |1        |\n",
      "|11 |2        |\n",
      "|90 |3        |\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_uni_df = spark.createDataFrame(employee_uni)\n",
    "employee_uni_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to show the unique ID of each user, If a user does not have a unique ID replace just show null.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|    name|unique_id|\n",
      "+--------+---------+\n",
      "|     Bob|     NULL|\n",
      "|   Alice|     NULL|\n",
      "|Jonathan|        1|\n",
      "|    Meir|        2|\n",
      "| Winston|        3|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.join(employee_uni_df, on=['id'], how='left').select(['name', 'unique_id']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Sales Analysis I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 100, 2008, 10, 5000], [2, 100, 2009, 12, 5000], [7, 200, 2011, 15, 9000]]\n",
    "sales = pd.DataFrame(data, columns=['sale_id', 'product_id', 'year', 'quantity', 'price']).astype({'sale_id':'Int64', 'product_id':'Int64', 'year':'Int64', 'quantity':'Int64', 'price':'Int64'})\n",
    "data = [[100, 'Nokia'], [200, 'Apple'], [300, 'Samsung']]\n",
    "product = pd.DataFrame(data, columns=['product_id', 'product_name']).astype({'product_id':'Int64', 'product_name':'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+--------+-----+\n",
      "|sale_id|product_id|year|quantity|price|\n",
      "+-------+----------+----+--------+-----+\n",
      "|1      |100       |2008|10      |5000 |\n",
      "|2      |100       |2009|12      |5000 |\n",
      "|7      |200       |2011|15      |9000 |\n",
      "+-------+----------+----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df = spark.createDataFrame(sales)\n",
    "sales_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|product_id|product_name|\n",
      "+----------+------------+\n",
      "|100       |Nokia       |\n",
      "|200       |Apple       |\n",
      "|300       |Samsung     |\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df = spark.createDataFrame(product)\n",
    "product_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to report the product_name, year, and price for each sale_id in the Sales table.\n",
    "\n",
    "Return the resulting table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+\n",
      "|product_name|year|price|\n",
      "+------------+----+-----+\n",
      "|       Nokia|2008| 5000|\n",
      "|       Nokia|2009| 5000|\n",
      "|       Apple|2011| 9000|\n",
      "+------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.join(product_df, on=['product_id'], how='left').select(['product_name', 'year', 'price']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Who Visited but Did Not Make Any Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 23], [2, 9], [4, 30], [5, 54], [6, 96], [7, 54], [8, 54]]\n",
    "visits = pd.DataFrame(data, columns=['visit_id', 'customer_id']).astype({'visit_id':'Int64', 'customer_id':'Int64'})\n",
    "data = [[2, 5, 310], [3, 5, 300], [9, 5, 200], [12, 1, 910], [13, 2, 970]]\n",
    "transactions = pd.DataFrame(data, columns=['transaction_id', 'visit_id', 'amount']).astype({'transaction_id':'Int64', 'visit_id':'Int64', 'amount':'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|visit_id|customer_id|\n",
      "+--------+-----------+\n",
      "|1       |23         |\n",
      "|2       |9          |\n",
      "|4       |30         |\n",
      "|5       |54         |\n",
      "|6       |96         |\n",
      "|7       |54         |\n",
      "|8       |54         |\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visits_df = spark.createDataFrame(visits)\n",
    "visits_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------+\n",
      "|transaction_id|visit_id|amount|\n",
      "+--------------+--------+------+\n",
      "|2             |5       |310   |\n",
      "|3             |5       |300   |\n",
      "|9             |5       |200   |\n",
      "|12            |1       |910   |\n",
      "|13            |2       |970   |\n",
      "+--------------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df = spark.createDataFrame(transactions)\n",
    "transactions_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find the IDs of the users who visited without making any transactions and the number of times they made these types of visits.\n",
    "\n",
    "Return the result table sorted in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|customer_id|count_no_trans|\n",
      "+-----------+--------------+\n",
      "|         54|             2|\n",
      "|         96|             1|\n",
      "|         30|             1|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "visits_df.join(transactions_df, on=['visit_id'], how='left').filter(F.col('transaction_id').isNull()).groupBy('customer_id').agg((F.count('customer_id')).alias('count_no_trans')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rising Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, '2015-01-01', 10], [2, '2015-01-02', 25], [3, '2015-01-03', 20], [4, '2015-01-04', 30]]\n",
    "weather = pd.DataFrame(data, columns=['id', 'recordDate', 'temperature']).astype({'id':'Int64', 'recordDate':'datetime64[ns]', 'temperature':'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------+\n",
      "|id |recordDate         |temperature|\n",
      "+---+-------------------+-----------+\n",
      "|1  |2015-01-01 00:00:00|10         |\n",
      "|2  |2015-01-02 00:00:00|25         |\n",
      "|3  |2015-01-03 00:00:00|20         |\n",
      "|4  |2015-01-04 00:00:00|30         |\n",
      "+---+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df = spark.createDataFrame(weather)\n",
    "weather_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find all dates' Id with higher temperatures compared to its previous dates (yesterday).\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.orderBy(\"recordDate\")\n",
    "weather_with_lag = weather_df.withColumn(\n",
    "    \"prev_temp\",\n",
    "    F.lag(\"temperature\").over(window_spec)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------+---------+\n",
      "| id|         recordDate|temperature|prev_temp|\n",
      "+---+-------------------+-----------+---------+\n",
      "|  1|2015-01-01 00:00:00|         10|     NULL|\n",
      "|  2|2015-01-02 00:00:00|         25|       10|\n",
      "|  3|2015-01-03 00:00:00|         20|       25|\n",
      "|  4|2015-01-04 00:00:00|         30|       20|\n",
      "+---+-------------------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_with_lag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  2|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_with_lag.filter(weather_with_lag['temperature'] > weather_with_lag['prev_temp']).select('id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Time of Process per Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[0, 0, 'start', 0.712], [0, 0, 'end', 1.52], [0, 1, 'start', 3.14], [0, 1, 'end', 4.12], [1, 0, 'start', 0.55],\n",
    "        [1, 0, 'end', 1.55], [1, 1, 'start', 0.43], [1, 1, 'end', 1.42], [2, 0, 'start', 4.1], [2, 0, 'end', 4.512],\n",
    "        [2, 1, 'start', 2.5], [2, 1, 'end', 5]]\n",
    "activity = pd.DataFrame(data, columns=['machine_id', 'process_id', 'activity_type', 'timestamp']).astype(\n",
    "    {'machine_id': 'Int64', 'process_id': 'Int64', 'activity_type': 'object', 'timestamp': 'Float64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------+---------+\n",
      "|machine_id|process_id|activity_type|timestamp|\n",
      "+----------+----------+-------------+---------+\n",
      "|         0|         0|        start|    0.712|\n",
      "|         0|         0|          end|     1.52|\n",
      "|         0|         1|        start|     3.14|\n",
      "|         0|         1|          end|     4.12|\n",
      "|         1|         0|        start|     0.55|\n",
      "|         1|         0|          end|     1.55|\n",
      "|         1|         1|        start|     0.43|\n",
      "|         1|         1|          end|     1.42|\n",
      "|         2|         0|        start|      4.1|\n",
      "|         2|         0|          end|    4.512|\n",
      "|         2|         1|        start|      2.5|\n",
      "|         2|         1|          end|      5.0|\n",
      "+----------+----------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df = spark.createDataFrame(activity)\n",
    "activity_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"There is a factory website that has several machines each running the same number of processes. Write a solution to find the average time each machine takes to complete a process.\n",
    "\n",
    "The time to complete a process is the 'end' timestamp minus the 'start' timestamp. The average time is calculated by the total time to complete every process on the machine divided by the number of processes that were run.\n",
    "\n",
    "The resulting table should have the machine_id along with the average time as processing_time, which should be rounded to 3 decimal places.\n",
    "\n",
    "Return the result table in any order.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = activity_df.groupBy(\"machine_id\", \"process_id\") \\\n",
    "    .pivot(\"activity_type\", [\"start\", \"end\"]) \\\n",
    "    .agg(F.first(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+-----+\n",
      "|machine_id|process_id|start|  end|\n",
      "+----------+----------+-----+-----+\n",
      "|         1|         0| 0.55| 1.55|\n",
      "|         1|         1| 0.43| 1.42|\n",
      "|         0|         1| 3.14| 4.12|\n",
      "|         2|         0|  4.1|4.512|\n",
      "|         0|         0|0.712| 1.52|\n",
      "|         2|         1|  2.5|  5.0|\n",
      "+----------+----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_df = pivot_df.withColumn(\n",
    "    \"processing_time\", \n",
    "    F.col(\"end\") - F.col(\"start\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+-----+------------------+\n",
      "|machine_id|process_id|start|  end|   processing_time|\n",
      "+----------+----------+-----+-----+------------------+\n",
      "|         1|         0| 0.55| 1.55|               1.0|\n",
      "|         1|         1| 0.43| 1.42|              0.99|\n",
      "|         0|         1| 3.14| 4.12|              0.98|\n",
      "|         2|         0|  4.1|4.512|0.4119999999999999|\n",
      "|         0|         0|0.712| 1.52|             0.808|\n",
      "|         2|         1|  2.5|  5.0|               2.5|\n",
      "+----------+----------+-----+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processing_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|machine_id|processing_time_avg|\n",
      "+----------+-------------------+\n",
      "|         0|              0.894|\n",
      "|         1|              0.995|\n",
      "|         2|              1.456|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processing_df.groupBy('machine_id').agg(F.round(F.avg('processing_time'), 3).alias('processing_time_avg')).select(['machine_id', 'processing_time_avg']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employee Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[3, 'Brad', None, 4000], [1, 'John', 3, 1000], [2, 'Dan', 3, 2000], [4, 'Thomas', 3, 4000]]\n",
    "employee = pd.DataFrame(data, columns=['empId', 'name', 'supervisor', 'salary']).astype(\n",
    "    {'empId': 'Int64', 'name': 'object', 'supervisor': 'Int64', 'salary': 'Int64'})\n",
    "data2 = [[2, 500], [4, 2000]]\n",
    "bonus = pd.DataFrame(data2, columns=['empId', 'bonus']).astype({'empId': 'Int64', 'bonus': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema_employee = StructType([\n",
    "    StructField(\"empId\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"supervisor\", IntegerType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "employee_df = spark.createDataFrame(data, schema_employee)\n",
    "bonus_df = spark.createDataFrame(bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+------+\n",
      "|empId|  name|supervisor|salary|\n",
      "+-----+------+----------+------+\n",
      "|    3|  Brad|      NULL|  4000|\n",
      "|    1|  John|         3|  1000|\n",
      "|    2|   Dan|         3|  2000|\n",
      "|    4|Thomas|         3|  4000|\n",
      "+-----+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|empId|bonus|\n",
      "+-----+-----+\n",
      "|    2|  500|\n",
      "|    4| 2000|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bonus_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to report the name and bonus amount of each employee with a bonus less than 1000.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|bonus|\n",
      "+----+-----+\n",
      "|John| NULL|\n",
      "|Brad| NULL|\n",
      "| Dan|  500|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.join(bonus_df, on='empId', how='left').filter((F.col('bonus') < 1000) | (F.col('bonus').isNull())).select(['name', 'bonus']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students and Examinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'Alice'], [2, 'Bob'], [13, 'John'], [6, 'Alex']]\n",
    "students = pd.DataFrame(data, columns=['student_id', 'student_name']).astype(\n",
    "    {'student_id': 'Int64', 'student_name': 'object'})\n",
    "data = [['Math'], ['Physics'], ['Programming']]\n",
    "subjects = pd.DataFrame(data, columns=['subject_name']).astype({'subject_name': 'object'})\n",
    "data = [[1, 'Math'], [1, 'Physics'], [1, 'Programming'], [2, 'Programming'], [1, 'Physics'], [1, 'Math'], [13, 'Math'],\n",
    "        [13, 'Programming'], [13, 'Physics'], [2, 'Math'], [1, 'Math']]\n",
    "examinations = pd.DataFrame(data, columns=['student_id', 'subject_name']).astype(\n",
    "    {'student_id': 'Int64', 'subject_name': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "students_df = spark.createDataFrame(students)\n",
    "subjects_df = spark.createDataFrame(subjects)\n",
    "examinations_df = spark.createDataFrame(examinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Write a solution to find the number of times each student attended each exam.\n",
    "\n",
    "Return the result table ordered by student_id and subject_name.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|student_id|student_name|\n",
      "+----------+------------+\n",
      "|         1|       Alice|\n",
      "|         2|         Bob|\n",
      "|        13|        John|\n",
      "|         6|        Alex|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|subject_name|\n",
      "+------------+\n",
      "|        Math|\n",
      "|     Physics|\n",
      "| Programming|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subjects_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|student_id|subject_name|\n",
      "+----------+------------+\n",
      "|         1|        Math|\n",
      "|         1|     Physics|\n",
      "|         1| Programming|\n",
      "|         2| Programming|\n",
      "|         1|     Physics|\n",
      "|         1|        Math|\n",
      "|        13|        Math|\n",
      "|        13| Programming|\n",
      "|        13|     Physics|\n",
      "|         2|        Math|\n",
      "|         1|        Math|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examinations_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "students_subjects = students_df.join(subjects_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+\n",
      "|student_id|student_name|subject_name|\n",
      "+----------+------------+------------+\n",
      "|         1|       Alice|        Math|\n",
      "|         1|       Alice|     Physics|\n",
      "|         1|       Alice| Programming|\n",
      "|         2|         Bob|        Math|\n",
      "|         2|         Bob|     Physics|\n",
      "|         2|         Bob| Programming|\n",
      "|        13|        John|        Math|\n",
      "|        13|        John|     Physics|\n",
      "|        13|        John| Programming|\n",
      "|         6|        Alex|        Math|\n",
      "|         6|        Alex|     Physics|\n",
      "|         6|        Alex| Programming|\n",
      "+----------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_subjects.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "students_exams = students_subjects.join(examinations_df, on=['student_id', 'subject_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+\n",
      "|student_id|subject_name|student_name|\n",
      "+----------+------------+------------+\n",
      "|        13| Programming|        John|\n",
      "|         2| Programming|         Bob|\n",
      "|         1| Programming|       Alice|\n",
      "|        13|        Math|        John|\n",
      "|         2|        Math|         Bob|\n",
      "|         1|        Math|       Alice|\n",
      "|         1|        Math|       Alice|\n",
      "|         1|        Math|       Alice|\n",
      "|        13|     Physics|        John|\n",
      "|         1|     Physics|       Alice|\n",
      "|         1|     Physics|       Alice|\n",
      "+----------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_exams.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "exams_count = students_exams.withColumn('exam_count',F.when(F.isnull('student_id'),0).otherwise(1)).groupBy(['student_id', 'subject_name', 'student_name']).agg(F.sum('exam_count').alias('attended_exams'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+--------------+\n",
      "|student_id|subject_name|student_name|attended_exams|\n",
      "+----------+------------+------------+--------------+\n",
      "|        13| Programming|        John|             1|\n",
      "|         2| Programming|         Bob|             1|\n",
      "|         1| Programming|       Alice|             1|\n",
      "|        13|        Math|        John|             1|\n",
      "|         2|        Math|         Bob|             1|\n",
      "|         1|        Math|       Alice|             3|\n",
      "|        13|     Physics|        John|             1|\n",
      "|         1|     Physics|       Alice|             2|\n",
      "+----------+------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exams_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+--------------+\n",
      "|student_id|subject_name|student_name|attended_exams|\n",
      "+----------+------------+------------+--------------+\n",
      "|         1|        Math|       Alice|             3|\n",
      "|         1|     Physics|       Alice|             2|\n",
      "|         1| Programming|       Alice|             1|\n",
      "|         2|        Math|         Bob|             1|\n",
      "|         2|     Physics|         Bob|             0|\n",
      "|         2| Programming|         Bob|             1|\n",
      "|         6|        Math|        Alex|             0|\n",
      "|         6|     Physics|        Alex|             0|\n",
      "|         6| Programming|        Alex|             0|\n",
      "|        13|        Math|        John|             1|\n",
      "|        13|     Physics|        John|             1|\n",
      "|        13| Programming|        John|             1|\n",
      "+----------+------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_subjects.join(exams_count, on=['student_id', 'subject_name', 'student_name'], how='left').na.fill(0, 'attended_exams').orderBy(['student_id', 'subject_name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managers with at Least 5 Direct Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[101, 'John', 'A', None], [102, 'Dan', 'A', 101], [103, 'James', 'A', 101], [104, 'Amy', 'A', 101], [105, 'Anne', 'A', 101], [106, 'Ron', 'B', 101]]\n",
    "employee = pd.DataFrame(data, columns=['id', 'name', 'department', 'managerId']).astype({'id':'Int64', 'name':'object', 'department':'object', 'managerId':'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"managerId\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "employee_df = spark.createDataFrame(data,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+---------+\n",
      "| id| name|department|managerId|\n",
      "+---+-----+----------+---------+\n",
      "|101| John|         A|     NULL|\n",
      "|102|  Dan|         A|      101|\n",
      "|103|James|         A|      101|\n",
      "|104|  Amy|         A|      101|\n",
      "|105| Anne|         A|      101|\n",
      "|106|  Ron|         B|      101|\n",
      "+---+-----+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find managers with at least five direct reports.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_count = employee_df.withColumn('report_count',F.when(F.isnull('managerId'),0).otherwise(1)).groupBy(['managerId']).agg(F.sum('report_count').alias('report_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|managerId|report_count|\n",
      "+---------+------------+\n",
      "|      101|           5|\n",
      "|     NULL|           0|\n",
      "+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|John|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.join(report_count, employee_df.id == report_count.managerId, \"left\").filter(F.col('report_count') >= 5).select('name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirmation Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[3, '2020-03-21 10:16:13'], [7, '2020-01-04 13:57:59'], [2, '2020-07-29 23:09:44'], [6, '2020-12-09 10:39:37']]\n",
    "signups = pd.DataFrame(data, columns=['user_id', 'time_stamp']).astype(\n",
    "    {'user_id': 'Int64', 'time_stamp': 'datetime64[ns]'})\n",
    "data = [[3, '2021-01-06 03:30:46', 'timeout'], [3, '2021-07-14 14:00:00', 'timeout'],\n",
    "        [7, '2021-06-12 11:57:29', 'confirmed'], [7, '2021-06-13 12:58:28', 'confirmed'],\n",
    "        [7, '2021-06-14 13:59:27', 'confirmed'], [2, '2021-01-22 00:00:00', 'confirmed'],\n",
    "        [2, '2021-02-28 23:59:59', 'timeout']]\n",
    "confirmations = pd.DataFrame(data, columns=['user_id', 'time_stamp', 'action']).astype(\n",
    "    {'user_id': 'Int64', 'time_stamp': 'datetime64[ns]', 'action': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "signups_df = spark.createDataFrame(signups)\n",
    "confirmations_df = spark.createDataFrame(confirmations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|user_id|         time_stamp|\n",
      "+-------+-------------------+\n",
      "|      3|2020-03-21 10:16:13|\n",
      "|      7|2020-01-04 13:57:59|\n",
      "|      2|2020-07-29 23:09:44|\n",
      "|      6|2020-12-09 10:39:37|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "signups_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------+\n",
      "|user_id|         time_stamp|   action|\n",
      "+-------+-------------------+---------+\n",
      "|      3|2021-01-06 03:30:46|  timeout|\n",
      "|      3|2021-07-14 14:00:00|  timeout|\n",
      "|      7|2021-06-12 11:57:29|confirmed|\n",
      "|      7|2021-06-13 12:58:28|confirmed|\n",
      "|      7|2021-06-14 13:59:27|confirmed|\n",
      "|      2|2021-01-22 00:00:00|confirmed|\n",
      "|      2|2021-02-28 23:59:59|  timeout|\n",
      "+-------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confirmations_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The confirmation rate of a user is the number of 'confirmed' messages divided by the total number of requested confirmation messages. The confirmation rate of a user that did not request any confirmation messages is 0. Round the confirmation rate to two decimal places.\n",
    "\n",
    "Write a solution to find the confirmation rate of each user.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_confirmations = signups_df.join(confirmations_df, on='user_id', how='left').na.fill('timeout', 'action')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+---------+\n",
      "|user_id|         time_stamp|         time_stamp|   action|\n",
      "+-------+-------------------+-------------------+---------+\n",
      "|      7|2020-01-04 13:57:59|2021-06-14 13:59:27|confirmed|\n",
      "|      7|2020-01-04 13:57:59|2021-06-13 12:58:28|confirmed|\n",
      "|      7|2020-01-04 13:57:59|2021-06-12 11:57:29|confirmed|\n",
      "|      6|2020-12-09 10:39:37|               NULL|  timeout|\n",
      "|      3|2020-03-21 10:16:13|2021-07-14 14:00:00|  timeout|\n",
      "|      3|2020-03-21 10:16:13|2021-01-06 03:30:46|  timeout|\n",
      "|      2|2020-07-29 23:09:44|2021-02-28 23:59:59|  timeout|\n",
      "|      2|2020-07-29 23:09:44|2021-01-22 00:00:00|confirmed|\n",
      "+-------+-------------------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_confirmations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_confirmations_count = user_confirmations.groupBy('user_id').pivot('action', ['confirmed', 'timeout']).agg(F.count('action')).na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------+\n",
      "|user_id|confirmed|timeout|\n",
      "+-------+---------+-------+\n",
      "|      7|        3|      0|\n",
      "|      6|        0|      1|\n",
      "|      3|        0|      2|\n",
      "|      2|        1|      1|\n",
      "+-------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_confirmations_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_confirmations_total = user_confirmations_count.withColumn('total', user_confirmations_count['confirmed'] + user_confirmations_count['timeout'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------+-----+\n",
      "|user_id|confirmed|timeout|total|\n",
      "+-------+---------+-------+-----+\n",
      "|      7|        3|      0|    3|\n",
      "|      6|        0|      1|    1|\n",
      "|      3|        0|      2|    2|\n",
      "|      2|        1|      1|    2|\n",
      "+-------+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_confirmations_total.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|user_id|confirmation_rate|\n",
      "+-------+-----------------+\n",
      "|      7|              1.0|\n",
      "|      6|              0.0|\n",
      "|      3|              0.0|\n",
      "|      2|              0.5|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_confirmations_total.withColumn('confirmation_rate', F.round(F.col('confirmed') / F.col('total'), 2)).select(['user_id', 'confirmation_rate']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Aggregate Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not Boring Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'War', 'great 3D', 8.9], [2, 'Science', 'fiction', 8.5], [3, 'irish', 'boring', 6.2],\n",
    "        [4, 'Ice song', 'Fantacy', 8.6], [5, 'House card', 'Interesting', 9.1]]\n",
    "cinema = pd.DataFrame(data, columns=['id', 'movie', 'description', 'rating']).astype(\n",
    "    {'id': 'Int64', 'movie': 'object', 'description': 'object', 'rating': 'Float64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "cinema_df = spark.createDataFrame(cinema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| id|     movie|description|rating|\n",
      "+---+----------+-----------+------+\n",
      "|  1|       War|   great 3D|   8.9|\n",
      "|  2|   Science|    fiction|   8.5|\n",
      "|  3|     irish|     boring|   6.2|\n",
      "|  4|  Ice song|    Fantacy|   8.6|\n",
      "|  5|House card|Interesting|   9.1|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cinema_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to report the movies with an odd-numbered ID and a description that is not \"boring\".\n",
    "\n",
    "Return the result table ordered by rating in descending order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| id|     movie|description|rating|\n",
      "+---+----------+-----------+------+\n",
      "|  5|House card|Interesting|   9.1|\n",
      "|  1|       War|   great 3D|   8.9|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cinema_df.filter((cinema_df['description'] != 'boring') & (F.col('id')%2 != 0)).orderBy('rating', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Selling Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, '2019-02-17', '2019-02-28', 5], [1, '2019-03-01', '2019-03-22', 20], [2, '2019-02-01', '2019-02-20', 15],\n",
    "        [2, '2019-02-21', '2019-03-31', 30]]\n",
    "prices = pd.DataFrame(data, columns=['product_id', 'start_date', 'end_date', 'price']).astype(\n",
    "    {'product_id': 'Int64', 'start_date': 'datetime64[ns]', 'end_date': 'datetime64[ns]', 'price': 'Int64'})\n",
    "data = [[1, '2019-02-25', 100], [1, '2019-03-01', 15], [2, '2019-02-10', 200], [2, '2019-03-22', 30]]\n",
    "units_sold = pd.DataFrame(data, columns=['product_id', 'purchase_date', 'units']).astype(\n",
    "    {'product_id': 'Int64', 'purchase_date': 'datetime64[ns]', 'units': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_df = spark.createDataFrame(prices)\n",
    "units_sold_df = spark.createDataFrame(units_sold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-----+\n",
      "|product_id|         start_date|           end_date|price|\n",
      "+----------+-------------------+-------------------+-----+\n",
      "|         1|2019-02-17 00:00:00|2019-02-28 00:00:00|    5|\n",
      "|         1|2019-03-01 00:00:00|2019-03-22 00:00:00|   20|\n",
      "|         2|2019-02-01 00:00:00|2019-02-20 00:00:00|   15|\n",
      "|         2|2019-02-21 00:00:00|2019-03-31 00:00:00|   30|\n",
      "+----------+-------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prices_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-----+\n",
      "|product_id|      purchase_date|units|\n",
      "+----------+-------------------+-----+\n",
      "|         1|2019-02-25 00:00:00|  100|\n",
      "|         1|2019-03-01 00:00:00|   15|\n",
      "|         2|2019-02-10 00:00:00|  200|\n",
      "|         2|2019-03-22 00:00:00|   30|\n",
      "+----------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "units_sold_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find the average selling price for each product. average_price should be rounded to 2 decimal places.\n",
    "\n",
    "Return the result table in any order\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-----+-------------------+-----+\n",
      "|product_id|         start_date|           end_date|price|      purchase_date|units|\n",
      "+----------+-------------------+-------------------+-----+-------------------+-----+\n",
      "|         1|2019-02-17 00:00:00|2019-02-28 00:00:00|    5|2019-02-25 00:00:00|  100|\n",
      "|         1|2019-02-17 00:00:00|2019-02-28 00:00:00|    5|2019-03-01 00:00:00|   15|\n",
      "|         1|2019-03-01 00:00:00|2019-03-22 00:00:00|   20|2019-02-25 00:00:00|  100|\n",
      "|         1|2019-03-01 00:00:00|2019-03-22 00:00:00|   20|2019-03-01 00:00:00|   15|\n",
      "|         2|2019-02-01 00:00:00|2019-02-20 00:00:00|   15|2019-02-10 00:00:00|  200|\n",
      "|         2|2019-02-01 00:00:00|2019-02-20 00:00:00|   15|2019-03-22 00:00:00|   30|\n",
      "|         2|2019-02-21 00:00:00|2019-03-31 00:00:00|   30|2019-02-10 00:00:00|  200|\n",
      "|         2|2019-02-21 00:00:00|2019-03-31 00:00:00|   30|2019-03-22 00:00:00|   30|\n",
      "+----------+-------------------+-------------------+-----+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prices_df.join(units_sold_df, on='product_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = prices_df.join(units_sold_df, on='product_id').filter((F.col('purchase_date') >= F.col('start_date')) & (F.col('purchase_date') < F.col('end_date')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-----+-------------------+-----+\n",
      "|product_id|         start_date|           end_date|price|      purchase_date|units|\n",
      "+----------+-------------------+-------------------+-----+-------------------+-----+\n",
      "|         1|2019-02-17 00:00:00|2019-02-28 00:00:00|    5|2019-02-25 00:00:00|  100|\n",
      "|         1|2019-03-01 00:00:00|2019-03-22 00:00:00|   20|2019-03-01 00:00:00|   15|\n",
      "|         2|2019-02-01 00:00:00|2019-02-20 00:00:00|   15|2019-02-10 00:00:00|  200|\n",
      "|         2|2019-02-21 00:00:00|2019-03-31 00:00:00|   30|2019-03-22 00:00:00|   30|\n",
      "+----------+-------------------+-------------------+-----+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|product_id|average_price|\n",
      "+----------+-------------+\n",
      "|         1|         6.96|\n",
      "|         2|        16.96|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.groupBy('product_id').agg(F.round(F.sum(F.col('price') * F.col('units')) / F.sum('units'), 2).alias('average_price')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Employees I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 1], [1, 2], [1, 3], [2, 1], [2, 4]]\n",
    "project = pd.DataFrame(data, columns=['project_id', 'employee_id']).astype(\n",
    "    {'project_id': 'Int64', 'employee_id': 'Int64'})\n",
    "data = [[1, 'Khaled', 3], [2, 'Ali', 2], [3, 'John', 1], [4, 'Doe', 2]]\n",
    "employee = pd.DataFrame(data, columns=['employee_id', 'name', 'experience_years']).astype(\n",
    "    {'employee_id': 'Int64', 'name': 'object', 'experience_years': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_df = spark.createDataFrame(project)\n",
    "employee_df = spark.createDataFrame(employee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|project_id|employee_id|\n",
      "+----------+-----------+\n",
      "|         1|          1|\n",
      "|         1|          2|\n",
      "|         1|          3|\n",
      "|         2|          1|\n",
      "|         2|          4|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "project_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----------------+\n",
      "|employee_id|  name|experience_years|\n",
      "+-----------+------+----------------+\n",
      "|          1|Khaled|               3|\n",
      "|          2|   Ali|               2|\n",
      "|          3|  John|               1|\n",
      "|          4|   Doe|               2|\n",
      "+-----------+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Write an SQL query that reports the average experience years of all the employees for each project, rounded to 2 digits.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = project_df.join(employee_df, on='employee_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+----------------+\n",
      "|employee_id|project_id|  name|experience_years|\n",
      "+-----------+----------+------+----------------+\n",
      "|          1|         1|Khaled|               3|\n",
      "|          1|         2|Khaled|               3|\n",
      "|          3|         1|  John|               1|\n",
      "|          2|         1|   Ali|               2|\n",
      "|          4|         2|   Doe|               2|\n",
      "+-----------+----------+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|project_id|average_years|\n",
      "+----------+-------------+\n",
      "|         1|          2.0|\n",
      "|         2|          2.5|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp.groupBy('project_id').agg(F.round(F.avg(F.col('experience_years')), 2).alias('average_years')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage of Users Attended a Contest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[6, 'Alice'], [2, 'Bob'], [7, 'Alex']]\n",
    "users = pd.DataFrame(data, columns=['user_id', 'user_name']).astype({'user_id': 'Int64', 'user_name': 'object'})\n",
    "data = [[215, 6], [209, 2], [208, 2], [210, 6], [208, 6], [209, 7], [209, 6], [215, 7], [208, 7], [210, 2], [207, 2],\n",
    "        [210, 7]]\n",
    "register = pd.DataFrame(data, columns=['contest_id', 'user_id']).astype({'contest_id': 'Int64', 'user_id': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = spark.createDataFrame(users)\n",
    "register_df = spark.createDataFrame(register)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|user_id|user_name|\n",
      "+-------+---------+\n",
      "|      6|    Alice|\n",
      "|      2|      Bob|\n",
      "|      7|     Alex|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|contest_id|user_id|\n",
      "+----------+-------+\n",
      "|       215|      6|\n",
      "|       209|      2|\n",
      "|       208|      2|\n",
      "|       210|      6|\n",
      "|       208|      6|\n",
      "|       209|      7|\n",
      "|       209|      6|\n",
      "|       215|      7|\n",
      "|       208|      7|\n",
      "|       210|      2|\n",
      "|       207|      2|\n",
      "|       210|      7|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "register_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find the percentage of the users registered in each contest rounded to two decimals.\n",
    "\n",
    "Return the result table ordered by percentage in descending order. In case of a tie, order it by contest_id in ascending order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|contest_id|user_id|\n",
      "+----------+-------+\n",
      "|       207|      2|\n",
      "|       208|      2|\n",
      "|       208|      6|\n",
      "|       208|      7|\n",
      "|       209|      2|\n",
      "|       209|      7|\n",
      "|       209|      6|\n",
      "|       210|      6|\n",
      "|       210|      2|\n",
      "|       210|      7|\n",
      "|       215|      6|\n",
      "|       215|      7|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "register_df.orderBy('contest_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|contest_id|user_count|\n",
      "+----------+----------+\n",
      "|       215|         2|\n",
      "|       209|         3|\n",
      "|       208|         3|\n",
      "|       207|         1|\n",
      "|       210|         3|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "register_df.groupBy('contest_id').agg(F.count('user_id').alias('user_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_users = register_df.groupBy('contest_id').agg(F.count('user_id').alias('user_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|contest_id|user_count|\n",
      "+----------+----------+\n",
      "|       215|         2|\n",
      "|       209|         3|\n",
      "|       208|         3|\n",
      "|       207|         1|\n",
      "|       210|         3|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "register_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------+\n",
      "|contest_id|user_count|total_no_users|\n",
      "+----------+----------+--------------+\n",
      "|       215|         2|             3|\n",
      "|       209|         3|             3|\n",
      "|       208|         3|             3|\n",
      "|       207|         1|             3|\n",
      "|       210|         3|             3|\n",
      "+----------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "register_users.withColumn('total_no_users', F.lit(users_df.count())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|contest_id|percentage|\n",
      "+----------+----------+\n",
      "|       209|     100.0|\n",
      "|       208|     100.0|\n",
      "|       210|     100.0|\n",
      "|       215|     66.67|\n",
      "|       207|     33.33|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "register_users.withColumn('total_no_users', F.lit(users_df.count())).withColumn('percentage', F.round((F.col('user_count') / F.col('total_no_users'))*100, 2)).orderBy('percentage', ascending=False).select(['contest_id', 'percentage']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries Quality and Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['Dog', 'Golden Retriever', 1, 5], ['Dog', 'German Shepherd', 2, 5], ['Dog', 'Mule', 200, 1],\n",
    "        ['Cat', 'Shirazi', 5, 2], ['Cat', 'Siamese', 3, 3], ['Cat', 'Sphynx', 7, 4]]\n",
    "queries = pd.DataFrame(data, columns=['query_name', 'result', 'position', 'rating']).astype(\n",
    "    {'query_name': 'object', 'result': 'object', 'position': 'Int64', 'rating': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df = spark.createDataFrame(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------+------+\n",
      "|query_name|          result|position|rating|\n",
      "+----------+----------------+--------+------+\n",
      "|       Dog|Golden Retriever|       1|     5|\n",
      "|       Dog| German Shepherd|       2|     5|\n",
      "|       Dog|            Mule|     200|     1|\n",
      "|       Cat|         Shirazi|       5|     2|\n",
      "|       Cat|         Siamese|       3|     3|\n",
      "|       Cat|          Sphynx|       7|     4|\n",
      "+----------+----------------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Query with rating less than 3 is a poor query.\n",
    "\n",
    "We define query quality as:\n",
    "\n",
    "The average of the ratio between query rating and its position.\n",
    "\n",
    "We also define poor query percentage as:\n",
    "\n",
    "The percentage of all queries with rating less than 3.\n",
    "\n",
    "Write a solution to find each query_name, the quality and poor_query_percentage.\n",
    "\n",
    "Both quality and poor_query_percentage should be rounded to 2 decimal places.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|query_name|quality|\n",
      "+----------+-------+\n",
      "|       Cat|   0.66|\n",
      "|       Dog|    2.5|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries_df.groupBy('query_name').agg(F.round(F.avg(F.col('rating')/F.col('position')), 2).alias('quality')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------------------+\n",
      "|query_name|quality|poor_query_percentage|\n",
      "+----------+-------+---------------------+\n",
      "|       Cat|   0.66|                33.33|\n",
      "|       Dog|    2.5|                33.33|\n",
      "+----------+-------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries_df.groupBy('query_name').agg(F.round(F.avg(F.col('rating')/F.col('position')), 2).alias('quality'), F.round(F.avg(F.when(F.col('rating') < 3, 1).otherwise(0)) * 100, 2).alias('poor_query_percentage')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly Transactions I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[121, 'US', 'approved', 1000, '2018-12-18'], [122, 'US', 'declined', 2000, '2018-12-19'],\n",
    "        [123, 'US', 'approved', 2000, '2019-01-01'], [124, 'DE', 'approved', 2000, '2019-01-07']]\n",
    "transactions = pd.DataFrame(data, columns=['id', 'country', 'state', 'amount', 'trans_date']).astype(\n",
    "    {'id': 'Int64', 'country': 'object', 'state': 'object', 'amount': 'Int64', 'trans_date': 'datetime64[ns]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = spark.createDataFrame(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+------+-------------------+\n",
      "| id|country|   state|amount|         trans_date|\n",
      "+---+-------+--------+------+-------------------+\n",
      "|121|     US|approved|  1000|2018-12-18 00:00:00|\n",
      "|122|     US|declined|  2000|2018-12-19 00:00:00|\n",
      "|123|     US|approved|  2000|2019-01-01 00:00:00|\n",
      "|124|     DE|approved|  2000|2019-01-07 00:00:00|\n",
      "+---+-------+--------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write an SQL query to find for each month and country, the number of transactions and their total amount, the number of approved transactions and their total amount.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_month = transactions_df.withColumn('month', F.substring(F.col('trans_date'), 1, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+------+-------------------+-------+\n",
      "| id|country|   state|amount|         trans_date|  month|\n",
      "+---+-------+--------+------+-------------------+-------+\n",
      "|121|     US|approved|  1000|2018-12-18 00:00:00|2018-12|\n",
      "|122|     US|declined|  2000|2018-12-19 00:00:00|2018-12|\n",
      "|123|     US|approved|  2000|2019-01-01 00:00:00|2019-01|\n",
      "|124|     DE|approved|  2000|2019-01-07 00:00:00|2019-01|\n",
      "+---+-------+--------+------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transaction_month.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+----------------------------------------------+-----------+-------------------------------------------------+\n",
      "|  month|country|count(id)|count(CASE WHEN (state = approved) THEN 1 END)|sum(amount)|sum(CASE WHEN (state = approved) THEN amount END)|\n",
      "+-------+-------+---------+----------------------------------------------+-----------+-------------------------------------------------+\n",
      "|2019-01|     US|        1|                                             1|       2000|                                             2000|\n",
      "|2019-01|     DE|        1|                                             1|       2000|                                             2000|\n",
      "|2018-12|     US|        2|                                             1|       3000|                                             1000|\n",
      "+-------+-------+---------+----------------------------------------------+-----------+-------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transaction_month.groupBy(['month', 'country']).agg(F.count(F.col('id')), F.count(F.when(F.col('state') =='approved', 1)), F.sum(F.col('amount')), F.sum(F.when(F.col('state') =='approved', F.col('amount')))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immediate Food Delivery II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 1, '2019-08-01', '2019-08-02'], [2, 2, '2019-08-02', '2019-08-02'], [3, 1, '2019-08-11', '2019-08-12'],\n",
    "        [4, 3, '2019-08-24', '2019-08-24'], [5, 3, '2019-08-21', '2019-08-22'], [6, 2, '2019-08-11', '2019-08-13'],\n",
    "        [7, 4, '2019-08-09', '2019-08-09']]\n",
    "delivery = pd.DataFrame(data,\n",
    "                        columns=['delivery_id', 'customer_id', 'order_date', 'customer_pref_delivery_date']).astype(\n",
    "    {'delivery_id': 'Int64', 'customer_id': 'Int64', 'order_date': 'datetime64[ns]',\n",
    "     'customer_pref_delivery_date': 'datetime64[ns]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_df = spark.createDataFrame(delivery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-------------------+---------------------------+\n",
      "|delivery_id|customer_id|         order_date|customer_pref_delivery_date|\n",
      "+-----------+-----------+-------------------+---------------------------+\n",
      "|          1|          1|2019-08-01 00:00:00|        2019-08-02 00:00:00|\n",
      "|          2|          2|2019-08-02 00:00:00|        2019-08-02 00:00:00|\n",
      "|          3|          1|2019-08-11 00:00:00|        2019-08-12 00:00:00|\n",
      "|          4|          3|2019-08-24 00:00:00|        2019-08-24 00:00:00|\n",
      "|          5|          3|2019-08-21 00:00:00|        2019-08-22 00:00:00|\n",
      "|          6|          2|2019-08-11 00:00:00|        2019-08-13 00:00:00|\n",
      "|          7|          4|2019-08-09 00:00:00|        2019-08-09 00:00:00|\n",
      "+-----------+-----------+-------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delivery_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If the customer's preferred delivery date is the same as the order date, then the order is called immediate; otherwise, it is called scheduled.\n",
    "\n",
    "The first order of a customer is the order with the earliest order date that the customer made. It is guaranteed that a customer has precisely one first order.\n",
    "\n",
    "Write a solution to find the percentage of immediate orders in the first orders of all customers, rounded to 2 decimal places.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n",
    "delivery_ranked = delivery_df.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "\n",
    "delivery_ranked_filtered = delivery_ranked.withColumn('order_status', F.when(F.col('order_date') == F.col('customer_pref_delivery_date'), 'immediate').otherwise('scheduled')).orderBy(['customer_id', 'order_date']).filter(F.col('row_num') == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-------------------+---------------------------+-------+------------+\n",
      "|delivery_id|customer_id|         order_date|customer_pref_delivery_date|row_num|order_status|\n",
      "+-----------+-----------+-------------------+---------------------------+-------+------------+\n",
      "|          1|          1|2019-08-01 00:00:00|        2019-08-02 00:00:00|      1|   scheduled|\n",
      "|          2|          2|2019-08-02 00:00:00|        2019-08-02 00:00:00|      1|   immediate|\n",
      "|          5|          3|2019-08-21 00:00:00|        2019-08-22 00:00:00|      1|   scheduled|\n",
      "|          7|          4|2019-08-09 00:00:00|        2019-08-09 00:00:00|      1|   immediate|\n",
      "+-----------+-----------+-------------------+---------------------------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delivery_ranked_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+\n",
      "|round((avg(CASE WHEN (order_status = scheduled) THEN 1 ELSE 0 END) * 100), 2)|\n",
      "+-----------------------------------------------------------------------------+\n",
      "|                                                                         50.0|\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delivery_ranked_filtered.agg(F.round(F.avg(F.when(F.col('order_status') == 'scheduled', 1).otherwise(0))*100, 2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Play Analysis IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2, '2016-03-01', 5], [1, 2, '2016-03-02', 6], [2, 3, '2017-06-25', 1], [3, 1, '2016-03-02', 0],\n",
    "        [3, 4, '2018-07-03', 5]]\n",
    "activity = pd.DataFrame(data, columns=['player_id', 'device_id', 'event_date', 'games_played']).astype(\n",
    "    {'player_id': 'Int64', 'device_id': 'Int64', 'event_date': 'datetime64[ns]', 'games_played': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df = spark.createDataFrame(activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+------------+\n",
      "|player_id|device_id|         event_date|games_played|\n",
      "+---------+---------+-------------------+------------+\n",
      "|        1|        2|2016-03-01 00:00:00|           5|\n",
      "|        1|        2|2016-03-02 00:00:00|           6|\n",
      "|        2|        3|2017-06-25 00:00:00|           1|\n",
      "|        3|        1|2016-03-02 00:00:00|           0|\n",
      "|        3|        4|2018-07-03 00:00:00|           5|\n",
      "+---------+---------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to report the fraction of players that logged in again on the day after the day they first logged in, rounded to 2 decimal places. \n",
    "In other words, you need to count the number of players that logged in for at least two consecutive days starting from their first login date, then divide that number by the total number of players.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"player_id\").orderBy(\"event_date\")\n",
    "player_log = activity_df.withColumn(\"row_num\", F.row_number().over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+------------+-------+\n",
      "|player_id|device_id|         event_date|games_played|row_num|\n",
      "+---------+---------+-------------------+------------+-------+\n",
      "|        1|        2|2016-03-01 00:00:00|           5|      1|\n",
      "|        1|        2|2016-03-02 00:00:00|           6|      2|\n",
      "|        2|        3|2017-06-25 00:00:00|           1|      1|\n",
      "|        3|        1|2016-03-02 00:00:00|           0|      1|\n",
      "|        3|        4|2018-07-03 00:00:00|           5|      2|\n",
      "+---------+---------+-------------------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "player_log.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT player_id)|\n",
      "+-------------------------+\n",
      "|                        3|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_players = player_log.agg(F.countDistinct(F.col('player_id'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT player_id)|\n",
      "+-------------------------+\n",
      "|                        2|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loyal_players = player_log.filter(player_log['row_num'] == 2).agg(F.countDistinct(F.col('player_id'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_login_df = activity_df.groupBy(\"player_id\").agg(F.min(\"event_date\").alias(\"first_login_date\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n",
      "|player_id|   first_login_date|\n",
      "+---------+-------------------+\n",
      "|        1|2016-03-01 00:00:00|\n",
      "|        3|2016-03-02 00:00:00|\n",
      "|        2|2017-06-25 00:00:00|\n",
      "+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_login_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = activity_df.join(first_login_df, on=\"player_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+------------+-------------------+\n",
      "|player_id|device_id|         event_date|games_played|   first_login_date|\n",
      "+---------+---------+-------------------+------------+-------------------+\n",
      "|        1|        2|2016-03-01 00:00:00|           5|2016-03-01 00:00:00|\n",
      "|        1|        2|2016-03-02 00:00:00|           6|2016-03-01 00:00:00|\n",
      "|        3|        1|2016-03-02 00:00:00|           0|2016-03-02 00:00:00|\n",
      "|        3|        4|2018-07-03 00:00:00|           5|2016-03-02 00:00:00|\n",
      "|        2|        3|2017-06-25 00:00:00|           1|2017-06-25 00:00:00|\n",
      "+---------+---------+-------------------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "relogin_df = joined_df.withColumn(\"day_after_first_login\", F.date_add(F.col(\"first_login_date\"), 1)).filter(F.col(\"event_date\") == F.col(\"day_after_first_login\")).select(\"player_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_no = joined_df.select('player_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction = round(relogin_df / total_players, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting and Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Unique Subjects Taught by Each Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2, 3], [1, 2, 4], [1, 3, 3], [2, 1, 1], [2, 2, 1], [2, 3, 1], [2, 4, 1]]\n",
    "teacher = pd.DataFrame(data, columns=['teacher_id', 'subject_id', 'dept_id']).astype(\n",
    "    {'teacher_id': 'Int64', 'subject_id': 'Int64', 'dept_id': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_df = spark.createDataFrame(teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+\n",
      "|teacher_id|subject_id|dept_id|\n",
      "+----------+----------+-------+\n",
      "|         1|         2|      3|\n",
      "|         1|         2|      4|\n",
      "|         1|         3|      3|\n",
      "|         2|         1|      1|\n",
      "|         2|         2|      1|\n",
      "|         2|         3|      1|\n",
      "|         2|         4|      1|\n",
      "+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teacher_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to calculate the number of unique subjects each teacher teaches in the university.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|teacher_id|count|\n",
      "+----------+-----+\n",
      "|         1|    2|\n",
      "|         2|    4|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teacher_df.groupBy('teacher_id').agg(F.count_distinct('subject_id').alias('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Activity for the Past 30 Days I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 1, '2019-07-20', 'open_session'], [1, 1, '2019-07-20', 'scroll_down'], [1, 1, '2019-07-20', 'end_session'],\n",
    "        [2, 4, '2019-07-20', 'open_session'], [2, 4, '2019-07-21', 'send_message'], [2, 4, '2019-07-21', 'end_session'],\n",
    "        [3, 2, '2019-07-21', 'open_session'], [3, 2, '2019-07-21', 'send_message'], [3, 2, '2019-07-21', 'end_session'],\n",
    "        [4, 3, '2019-06-25', 'open_session'], [4, 3, '2019-06-25', 'end_session']]\n",
    "activity = pd.DataFrame(data, columns=['user_id', 'session_id', 'activity_date', 'activity_type']).astype(\n",
    "    {'user_id': 'Int64', 'session_id': 'Int64', 'activity_date': 'datetime64[ns]', 'activity_type': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df = spark.createDataFrame(activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------------+-------------+\n",
      "|user_id|session_id|      activity_date|activity_type|\n",
      "+-------+----------+-------------------+-------------+\n",
      "|      1|         1|2019-07-20 00:00:00| open_session|\n",
      "|      1|         1|2019-07-20 00:00:00|  scroll_down|\n",
      "|      1|         1|2019-07-20 00:00:00|  end_session|\n",
      "|      2|         4|2019-07-20 00:00:00| open_session|\n",
      "|      2|         4|2019-07-21 00:00:00| send_message|\n",
      "|      2|         4|2019-07-21 00:00:00|  end_session|\n",
      "|      3|         2|2019-07-21 00:00:00| open_session|\n",
      "|      3|         2|2019-07-21 00:00:00| send_message|\n",
      "|      3|         2|2019-07-21 00:00:00|  end_session|\n",
      "|      4|         3|2019-06-25 00:00:00| open_session|\n",
      "|      4|         3|2019-06-25 00:00:00|  end_session|\n",
      "+-------+----------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find the daily active user count for a period of 30 days ending 2019-07-27 inclusively. A user was active on someday if they made at least one activity on that day.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+\n",
      "|      activity_date|count(DISTINCT user_id)|\n",
      "+-------------------+-----------------------+\n",
      "|2019-07-21 00:00:00|                      2|\n",
      "|2019-07-20 00:00:00|                      2|\n",
      "+-------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df.withColumn('end_date', F.lit('2019-07-27').cast(\"timestamp\")).withColumn('start_date', F.date_add(F.col('end_date'), -30)).filter((F.col('activity_date') >= F.col('start_date')) & (F.col('activity_date') <= F.col('end_date'))).groupBy('activity_date').agg(F.count_distinct('user_id')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Sales Analysis III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 100, 2008, 10, 5000], [2, 100, 2009, 12, 5000], [7, 200, 2011, 15, 9000]]\n",
    "sales = pd.DataFrame(data, columns=['sale_id', 'product_id', 'year', 'quantity', 'price']).astype(\n",
    "    {'sale_id': 'Int64', 'product_id': 'Int64', 'year': 'Int64', 'quantity': 'Int64', 'price': 'Int64'})\n",
    "data = [[100, 'Nokia'], [200, 'Apple'], [300, 'Samsung']]\n",
    "product = pd.DataFrame(data, columns=['product_id', 'product_name']).astype(\n",
    "    {'product_id': 'Int64', 'product_name': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = spark.createDataFrame(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+--------+-----+\n",
      "|sale_id|product_id|year|quantity|price|\n",
      "+-------+----------+----+--------+-----+\n",
      "|      1|       100|2008|      10| 5000|\n",
      "|      2|       100|2009|      12| 5000|\n",
      "|      7|       200|2011|      15| 9000|\n",
      "+-------+----------+----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = spark.createDataFrame(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|product_id|product_name|\n",
      "+----------+------------+\n",
      "|       100|       Nokia|\n",
      "|       200|       Apple|\n",
      "|       300|     Samsung|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to select the product id, year, quantity, and price for the first year of every product sold.\n",
    "\n",
    "Return the resulting table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_first_year = sales_df.groupBy(\"product_id\").agg(F.min(\"year\").alias(\"first_year\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+-----+\n",
      "|product_id|year|quantity|price|\n",
      "+----------+----+--------+-----+\n",
      "|       100|2008|      10| 5000|\n",
      "|       200|2011|      15| 9000|\n",
      "+----------+----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.join(sales_first_year, on='product_id', how='left').filter(F.col('year') == F.col('first_year')).select(['product_id', 'year', 'quantity', 'price']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes More Than 5 Students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['A', 'Math'], ['B', 'English'], ['C', 'Math'], ['D', 'Biology'], ['E', 'Math'], ['F', 'Computer'],\n",
    "        ['G', 'Math'], ['H', 'Math'], ['I', 'Math']]\n",
    "courses = pd.DataFrame(data, columns=['student', 'class']).astype({'student': 'object', 'class': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_df = spark.createDataFrame(courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|student|   class|\n",
      "+-------+--------+\n",
      "|      A|    Math|\n",
      "|      B| English|\n",
      "|      C|    Math|\n",
      "|      D| Biology|\n",
      "|      E|    Math|\n",
      "|      F|Computer|\n",
      "|      G|    Math|\n",
      "|      H|    Math|\n",
      "|      I|    Math|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "courses_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find all the classes that have at least five students.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|class|\n",
      "+-----+\n",
      "| Math|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "courses_df.groupBy('class').agg(F.count_distinct(F.col('student')).alias('number_of_students')).filter(F.col('number_of_students') > 5).select('class').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Followers Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['0', '1'], ['1', '0'], ['2', '0'], ['2', '1']]\n",
    "followers = pd.DataFrame(data, columns=['user_id', 'follower_id']).astype({'user_id': 'Int64', 'follower_id': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_df = spark.createDataFrame(followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|user_id|follower_id|\n",
      "+-------+-----------+\n",
      "|      0|          1|\n",
      "|      1|          0|\n",
      "|      2|          0|\n",
      "|      2|          1|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "followers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution that will, for each user, return the number of followers.\n",
    "\n",
    "Return the result table ordered by user_id in ascending order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|user_id|no_followers|\n",
      "+-------+------------+\n",
      "|      0|           1|\n",
      "|      1|           1|\n",
      "|      2|           2|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "followers_df.groupBy('user_id').agg(F.count_distinct(F.col('follower_id')).alias('no_followers')).orderBy(F.col('no_followers')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biggest Single Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[8], [8], [3], [3], [1], [4], [5], [6]]  # first example\n",
    "#data = [[8], [8], [7], [7], [3], [3], [3]] # second example\n",
    "my_numbers = pd.DataFrame(data, columns=['num']).astype({'num': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_numbers_df = spark.createDataFrame(my_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "|  8|\n",
      "|  8|\n",
      "|  3|\n",
      "|  3|\n",
      "|  1|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_numbers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A single number is a number that appeared only once in the MyNumbers table.\n",
    "\n",
    "Find the largest single number. If there is no single number, report null.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|max|\n",
      "+---+\n",
      "|  6|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_numbers_df.groupBy('num').agg(F.count(F.col('num')).alias('count')).filter(F.col('count') == 1).agg(F.max(F.col('num')).alias('max')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customers Who Bought All Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 5], [2, 6], [3, 5], [3, 6], [1, 6]]\n",
    "customer = pd.DataFrame(data, columns=['customer_id', 'product_key']).astype(\n",
    "    {'customer_id': 'Int64', 'product_key': 'Int64'})\n",
    "data = [[5], [6]]\n",
    "product = pd.DataFrame(data, columns=['product_key']).astype({'product_key': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = spark.createDataFrame(customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|customer_id|product_key|\n",
      "+-----------+-----------+\n",
      "|          1|          5|\n",
      "|          2|          6|\n",
      "|          3|          5|\n",
      "|          3|          6|\n",
      "|          1|          6|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = spark.createDataFrame(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|product_key|\n",
      "+-----------+\n",
      "|          5|\n",
      "|          6|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to report the customer ids from the Customer table that bought all the products in the Product table.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_products = product_df.agg(F.count('product_key')).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|customer_id|no_products_bought|\n",
      "+-----------+------------------+\n",
      "|          1|                 2|\n",
      "|          3|                 2|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.groupBy('customer_id').agg(F.count_distinct(F.col('product_key')).alias('no_products_bought')).filter(F.col('no_products_bought') == number_of_products).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Select and Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Number of Employees Which Report to Each Employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[9, 'Hercy', None, 43], [6, 'Alice', 9, 41], [4, 'Bob', 9, 36], [2, 'Winston', None, 37]]\n",
    "employees = pd.DataFrame(data, columns=['employee_id', 'name', 'reports_to', 'age']) \\\n",
    "    .astype({'employee_id': 'Int64', 'name': 'object', 'reports_to': 'Int64', 'age': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"reports_to\", IntegerType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "employees_df = spark.createDataFrame(data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----------+---+\n",
      "|employee_id|   name|reports_to|age|\n",
      "+-----------+-------+----------+---+\n",
      "|          9|  Hercy|      NULL| 43|\n",
      "|          6|  Alice|         9| 41|\n",
      "|          4|    Bob|         9| 36|\n",
      "|          2|Winston|      NULL| 37|\n",
      "+-----------+-------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For this problem, we will consider a manager an employee who has at least 1 other employee reporting to them.\n",
    "\n",
    "Write a solution to report the ids and the names of all managers, the number of employees who report directly to them, and the average age of the reports rounded to the nearest integer.\n",
    "\n",
    "Return the result table ordered by employee_id.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_info = employees_df.filter(~employees_df['reports_to'].isNull()).groupBy('reports_to').agg(F.count_distinct(F.col('employee_id')).alias('no_emplyees'), F.round(F.avg(F.col('age')), 0).alias('avg_age'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------+\n",
      "|reports_to|no_emplyees|avg_age|\n",
      "+----------+-----------+-------+\n",
      "|         9|          2|   39.0|\n",
      "+----------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+----------+---+----------+-----------+-------+\n",
      "|employee_id| name|reports_to|age|reports_to|no_emplyees|avg_age|\n",
      "+-----------+-----+----------+---+----------+-----------+-------+\n",
      "|          9|Hercy|      NULL| 43|         9|          2|   39.0|\n",
      "+-----------+-----+----------+---+----------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.join(employees_info, employees_df['employee_id'] == employees_info['reports_to'], how='inner').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Department for Each Employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['1', '1', 'N'], ['2', '1', 'Y'], ['2', '2', 'N'], ['3', '3', 'N'], ['4', '2', 'N'], ['4', '3', 'Y'],\n",
    "        ['4', '4', 'N']]\n",
    "employee = pd.DataFrame(data, columns=['employee_id', 'department_id', 'primary_flag']).astype(\n",
    "    {'employee_id': 'Int64', 'department_id': 'Int64', 'primary_flag': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = spark.createDataFrame(employee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------+\n",
      "|employee_id|department_id|primary_flag|\n",
      "+-----------+-------------+------------+\n",
      "|          1|            1|           N|\n",
      "|          2|            1|           Y|\n",
      "|          2|            2|           N|\n",
      "|          3|            3|           N|\n",
      "|          4|            2|           N|\n",
      "|          4|            3|           Y|\n",
      "|          4|            4|           N|\n",
      "+-----------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Employees can belong to multiple departments. When the employee joins other departments, they need to decide which department is their primary department. Note that when an employee belongs to only one department, their primary column is 'N'.\n",
    "\n",
    "Write a solution to report all the employees with their primary department. For employees who belong to one department, report their only department.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"employee_id\").orderBy(F.desc(\"primary_flag\"), F.col(\"department_id\"))\n",
    "employee_history = employee_df.withColumn(\"department_order\", F.row_number().over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------+----------------+\n",
      "|employee_id|department_id|primary_flag|department_order|\n",
      "+-----------+-------------+------------+----------------+\n",
      "|          1|            1|           N|               1|\n",
      "|          2|            1|           Y|               1|\n",
      "|          2|            2|           N|               2|\n",
      "|          3|            3|           N|               1|\n",
      "|          4|            3|           Y|               1|\n",
      "|          4|            2|           N|               2|\n",
      "|          4|            4|           N|               3|\n",
      "+-----------+-------------+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_history.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------+----------------+\n",
      "|employee_id|department_id|primary_flag|department_order|\n",
      "+-----------+-------------+------------+----------------+\n",
      "|          1|            1|           N|               1|\n",
      "|          2|            1|           Y|               1|\n",
      "|          3|            3|           N|               1|\n",
      "|          4|            3|           Y|               1|\n",
      "+-----------+-------------+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_history.filter((F.col('department_order') == 1) | (F.col('primary_flag') == 'Y')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triangle Judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[13, 15, 30], [10, 20, 15]]\n",
    "triangle = pd.DataFrame(data, columns=['x', 'y', 'z']).astype({'x': 'Int64', 'y': 'Int64', 'z': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangle_df = spark.createDataFrame(triangle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  x|  y|  z|\n",
      "+---+---+---+\n",
      "| 13| 15| 30|\n",
      "| 10| 20| 15|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "triangle_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Report for every three line segments whether they can form a triangle.\n",
    "\n",
    "Return the result table in any order.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+\n",
      "|  x|  y|  z|traingle|\n",
      "+---+---+---+--------+\n",
      "| 13| 15| 30|      No|\n",
      "| 10| 20| 15|     Yes|\n",
      "+---+---+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "triangle_df.withColumn('traingle', F.when((F.col('x') + F.col('y') > F.col('z')) &\n",
    "                                          (F.col('x') + F.col('z') > F.col('y')) &\n",
    "                                          (F.col('z') + F.col('y') > F.col('x')), 'Yes').otherwise('No')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consecutive Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 1], [2, 1], [3, 1], [4, 2], [5, 1], [6, 2], [7, 2]]\n",
    "logs = pd.DataFrame(data, columns=['id', 'num']).astype({'id':'Int64', 'num':'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df = spark.createDataFrame(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|num|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "|  2|  1|\n",
      "|  3|  1|\n",
      "|  4|  2|\n",
      "|  5|  1|\n",
      "|  6|  2|\n",
      "|  7|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find all numbers that appear at least three times consecutively.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approach is to check the previous and 2nd prev value and if both are equal to val then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.orderBy(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df = logs_df.withColumn('prev1_val', F.lag(F.col('num'), 1).over(w))\n",
    "logs_df = logs_df.withColumn('prev2_val', F.lag(F.col('num'), 2).over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+---------+\n",
      "| id|num|prev1_val|prev2_val|\n",
      "+---+---+---------+---------+\n",
      "|  1|  1|     NULL|     NULL|\n",
      "|  2|  1|        1|     NULL|\n",
      "|  3|  1|        1|        1|\n",
      "|  4|  2|        1|        1|\n",
      "|  5|  1|        2|        1|\n",
      "|  6|  2|        1|        2|\n",
      "|  7|  2|        2|        1|\n",
      "+---+---+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|ConsecutiveNums|\n",
      "+---------------+\n",
      "|              1|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df.filter((F.col('num') == F.col('prev1_val')) & (F.col('num') == F.col('prev2_val'))).select(F.col('num').alias('ConsecutiveNums')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Price at a Given Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 20, '2019-08-14'], [2, 50, '2019-08-14'], [1, 30, '2019-08-15'], [1, 35, '2019-08-16'],\n",
    "        [2, 65, '2019-08-17'], [3, 20, '2019-08-18']]\n",
    "\n",
    "products = pd.DataFrame(data, columns=['product_id', 'new_price', 'change_date']).astype(\n",
    "    {'product_id': 'Int64', 'new_price': 'Int64', 'change_date': 'datetime64[ns]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = spark.createDataFrame(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------------+\n",
      "|product_id|new_price|        change_date|\n",
      "+----------+---------+-------------------+\n",
      "|         1|       20|2019-08-14 00:00:00|\n",
      "|         2|       50|2019-08-14 00:00:00|\n",
      "|         1|       30|2019-08-15 00:00:00|\n",
      "|         1|       35|2019-08-16 00:00:00|\n",
      "|         2|       65|2019-08-17 00:00:00|\n",
      "|         3|       20|2019-08-18 00:00:00|\n",
      "+----------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find the prices of all products on 2019-08-16. Assume the price of all products before any change is 10.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_products = products_df.filter(F.col('change_date') <= '2019-08-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------------+\n",
      "|product_id|new_price|        change_date|\n",
      "+----------+---------+-------------------+\n",
      "|         1|       20|2019-08-14 00:00:00|\n",
      "|         2|       50|2019-08-14 00:00:00|\n",
      "|         1|       30|2019-08-15 00:00:00|\n",
      "|         1|       35|2019-08-16 00:00:00|\n",
      "+----------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"product_id\").orderBy(F.col(\"change_date\").desc())\n",
    "latest_price = filtered_products.withColumn(\"latest_price_no\", F.row_number().over(window_spec)).filter(F.col('latest_price_no') == 1).withColumn('latest_price', F.col('new_price'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------------+---------------+------------+\n",
      "|product_id|new_price|        change_date|latest_price_no|latest_price|\n",
      "+----------+---------+-------------------+---------------+------------+\n",
      "|         1|       35|2019-08-16 00:00:00|              1|          35|\n",
      "|         2|       50|2019-08-14 00:00:00|              1|          50|\n",
      "+----------+---------+-------------------+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latest_price.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------------+\n",
      "|product_id|new_price|        change_date|\n",
      "+----------+---------+-------------------+\n",
      "|         1|       20|2019-08-14 00:00:00|\n",
      "|         2|       50|2019-08-14 00:00:00|\n",
      "|         1|       30|2019-08-15 00:00:00|\n",
      "|         1|       35|2019-08-16 00:00:00|\n",
      "|         2|       65|2019-08-17 00:00:00|\n",
      "|         3|       20|2019-08-18 00:00:00|\n",
      "+----------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_products_df = products_df.select('product_id').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|product_id|\n",
      "+----------+\n",
      "|         1|\n",
      "|         3|\n",
      "|         2|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_products_df_total = all_products_df.join(latest_price, on='product_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------------+---------------+------------+\n",
      "|product_id|new_price|        change_date|latest_price_no|latest_price|\n",
      "+----------+---------+-------------------+---------------+------------+\n",
      "|         1|       35|2019-08-16 00:00:00|              1|          35|\n",
      "|         3|     NULL|               NULL|           NULL|        NULL|\n",
      "|         2|       50|2019-08-14 00:00:00|              1|          50|\n",
      "+----------+---------+-------------------+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_products_df_total.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|product_id|latest_price|\n",
      "+----------+------------+\n",
      "|         1|          35|\n",
      "|         3|          10|\n",
      "|         2|          50|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_products_df_total.na.fill(10, 'latest_price').select(['product_id', 'latest_price']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Person to Fit in the Bus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[5, 'Alice', 250, 1], [4, 'Bob', 175, 5], [3, 'Alex', 350, 2], [6, 'John Cena', 400, 3], [1, 'Winston', 500, 6],\n",
    "        [2, 'Marie', 200, 4]]\n",
    "queue = pd.DataFrame(data, columns=['person_id', 'person_name', 'weight', 'turn']).astype(\n",
    "    {'person_id': 'Int64', 'person_name': 'object', 'weight': 'Int64', 'turn': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue_df = spark.createDataFrame(queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------+----+\n",
      "|person_id|person_name|weight|turn|\n",
      "+---------+-----------+------+----+\n",
      "|        5|      Alice|   250|   1|\n",
      "|        4|        Bob|   175|   5|\n",
      "|        3|       Alex|   350|   2|\n",
      "|        6|  John Cena|   400|   3|\n",
      "|        1|    Winston|   500|   6|\n",
      "|        2|      Marie|   200|   4|\n",
      "+---------+-----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queue_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "There is a queue of people waiting to board a bus. However, the bus has a weight limit of 1000 kilograms, so there may be some people who cannot board.\n",
    "\n",
    "Write a solution to find the person_name of the last person that can fit on the bus without exceeding the weight limit. The test cases are generated such that the first person does not exceed the weight limit.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------+----+\n",
      "|person_id|person_name|weight|turn|\n",
      "+---------+-----------+------+----+\n",
      "|        5|      Alice|   250|   1|\n",
      "|        3|       Alex|   350|   2|\n",
      "|        6|  John Cena|   400|   3|\n",
      "|        2|      Marie|   200|   4|\n",
      "|        4|        Bob|   175|   5|\n",
      "|        1|    Winston|   500|   6|\n",
      "+---------+-----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queue_df.orderBy('turn').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.orderBy(F.col(\"turn\"))\n",
    "cumsum_queue = queue_df.withColumn(\"cumsum\", F.sum(F.col('weight')).over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------+----+------+\n",
      "|person_id|person_name|weight|turn|cumsum|\n",
      "+---------+-----------+------+----+------+\n",
      "|        5|      Alice|   250|   1|   250|\n",
      "|        3|       Alex|   350|   2|   600|\n",
      "|        6|  John Cena|   400|   3|  1000|\n",
      "|        2|      Marie|   200|   4|  1200|\n",
      "|        4|        Bob|   175|   5|  1375|\n",
      "|        1|    Winston|   500|   6|  1875|\n",
      "+---------+-----------+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cumsum_queue.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|person_name|\n",
      "+-----------+\n",
      "|  John Cena|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cumsum_queue.filter(F.col('cumsum') <= 1000).orderBy(F.col('cumsum').desc()).select(F.col('person_name')).limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Salary Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[3, 108939], [2, 12747], [8, 87709], [6, 91796]]\n",
    "accounts = pd.DataFrame(data, columns=['account_id', 'income']).astype({'account_id':'Int64', 'income':'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df = spark.createDataFrame(accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|account_id|income|\n",
      "+----------+------+\n",
      "|         3|108939|\n",
      "|         2| 12747|\n",
      "|         8| 87709|\n",
      "|         6| 91796|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accounts_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to calculate the number of bank accounts for each salary category. The salary categories are:\n",
    "\n",
    "\"Low Salary\": All the salaries strictly less than 20000. \"Average Salary\": All the salaries in the inclusive range (20000, 50000). \"High Salary\": All the salaries strictly greater than 50000.\n",
    "\n",
    "The result table must contain all three categories. If there are no accounts in a category, return 0.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = accounts_df.withColumn('category', F.when(F.col('income') < 20000, 'Low Salary').when((F.col('income') >= 20000) & (F.col('income') <= 50000), 'Average Salary').when(F.col('income') > 50000, 'High Salary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----------+\n",
      "|account_id|income|   category|\n",
      "+----------+------+-----------+\n",
      "|         3|108939|High Salary|\n",
      "|         2| 12747| Low Salary|\n",
      "|         8| 87709|High Salary|\n",
      "|         6| 91796|High Salary|\n",
      "+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_summary = categories.groupBy('category').agg(F.count('category').alias('accounts_count '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+\n",
      "|   category|accounts_count |\n",
      "+-----------+---------------+\n",
      "|High Salary|              3|\n",
      "| Low Salary|              1|\n",
      "+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_categories = spark.createDataFrame([\n",
    "    Row(category='Low Salary'),\n",
    "    Row(category='Average Salary'),\n",
    "    Row(category='High Salary')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|      category|\n",
      "+--------------+\n",
      "|    Low Salary|\n",
      "|Average Salary|\n",
      "|   High Salary|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_categories.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = all_categories.join(categories_summary, on='category', how='left').na.fill(0, 'accounts_count ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+\n",
      "|      category|accounts_count |\n",
      "+--------------+---------------+\n",
      "|   High Salary|              3|\n",
      "|    Low Salary|              1|\n",
      "|Average Salary|              0|\n",
      "+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subqueries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employees Whose Manager Left the Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[3, 'Mila', 9, 60301], [12, 'Antonella', None, 31000], [13, 'Emery', None, 67084], [1, 'Kalel', 11, 21241],\n",
    "        [9, 'Mikaela', None, 50937], [11, 'Joziah', 6, 28485]]\n",
    "employees = pd.DataFrame(data, columns=['employee_id', 'name', 'manager_id', 'salary']).astype(\n",
    "    {'employee_id': 'Int64', 'name': 'object', 'manager_id': 'Int64', 'salary': 'Int64'})\n",
    "\n",
    "# # spark has issues with null in Int64 of pandas, hence converting to str for now. Will convert to int after loading in spark.\n",
    "employees['manager_id'] = employees['manager_id'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_df = spark.createDataFrame(employees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+------+\n",
      "|employee_id|     name|manager_id|salary|\n",
      "+-----------+---------+----------+------+\n",
      "|          3|     Mila|         9| 60301|\n",
      "|         12|Antonella|      <NA>| 31000|\n",
      "|         13|    Emery|      <NA>| 67084|\n",
      "|          1|    Kalel|        11| 21241|\n",
      "|          9|  Mikaela|      <NA>| 50937|\n",
      "|         11|   Joziah|         6| 28485|\n",
      "+-----------+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_df = employees_df.withColumn('manager_id', F.col('manager_id').try_cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+------+\n",
      "|employee_id|     name|manager_id|salary|\n",
      "+-----------+---------+----------+------+\n",
      "|          3|     Mila|         9| 60301|\n",
      "|         12|Antonella|      NULL| 31000|\n",
      "|         13|    Emery|      NULL| 67084|\n",
      "|          1|    Kalel|        11| 21241|\n",
      "|          9|  Mikaela|      NULL| 50937|\n",
      "|         11|   Joziah|         6| 28485|\n",
      "+-----------+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the IDs of the employees whose salary is strictly less than dollar 30000 and whose manager left the company. \n",
    "When a manager leaves the company, their information is deleted from the Employees table, but the reports still have their manager_id set to the manager that left.\n",
    "\n",
    "Return the result table ordered by employee_id.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_salary = employees_df.filter(F.col('salary') < 30000).select(['employee_id', 'manager_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|employee_id|manager_id|\n",
      "+-----------+----------+\n",
      "|          1|        11|\n",
      "|         11|         6|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "low_salary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_salary = employees_df.filter(F.col(\"salary\") < 30000)\n",
    "\n",
    "current_managers = employees_df.select(\"employee_id\").withColumnRenamed(\"employee_id\", \"manager_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|manager_id|\n",
      "+----------+\n",
      "|         3|\n",
      "|        12|\n",
      "|        13|\n",
      "|         1|\n",
      "|         9|\n",
      "|        11|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_managers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = low_salary.join(current_managers, on=\"manager_id\", how=\"left_anti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------+------+\n",
      "|manager_id|employee_id|  name|salary|\n",
      "+----------+-----------+------+------+\n",
      "|         6|         11|Joziah| 28485|\n",
      "+----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|employee_id|\n",
      "+-----------+\n",
      "|         11|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select(\"employee_id\").orderBy(\"employee_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exchange Seats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'Abbot'], [2, 'Doris'], [3, 'Emerson'], [4, 'Green'], [5, 'Jeames']]\n",
    "seat = pd.DataFrame(data, columns=['id', 'student']).astype({'id': 'Int64', 'student': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "seat_df = spark.createDataFrame(seat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|student|\n",
      "+---+-------+\n",
      "|  1|  Abbot|\n",
      "|  2|  Doris|\n",
      "|  3|Emerson|\n",
      "|  4|  Green|\n",
      "|  5| Jeames|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seat_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to swap the seat id of every two consecutive students. If the number of students is odd, the id of the last student is not swapped.\n",
    "\n",
    "Return the result table ordered by id in ascending order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = seat_df.agg(F.max('id')).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+\n",
      "| id|student|id_fixed|\n",
      "+---+-------+--------+\n",
      "|  1|  Abbot|       1|\n",
      "|  2|  Doris|       0|\n",
      "|  3|Emerson|       1|\n",
      "|  4|  Green|       0|\n",
      "|  5| Jeames|       1|\n",
      "+---+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seat_df.withColumn('id_fixed', F.col('id') % 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|student|\n",
      "+---+-------+\n",
      "|  1|  Doris|\n",
      "|  2|  Abbot|\n",
      "|  3|  Green|\n",
      "|  4|Emerson|\n",
      "|  5| Jeames|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seat_df.withColumn('id_fixed', F.when(F.col('id') % 2 == 0, F.col('id')-1).when(F.col('id') == max_val, F.col('id')).otherwise(F.col('id') + 1)).select(['id_fixed', 'student']).orderBy('id_fixed').withColumnRenamed('id_fixed', 'id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'Avengers'], [2, 'Frozen 2'], [3, 'Joker']]\n",
    "movies = pd.DataFrame(data, columns=['movie_id', 'title']).astype({'movie_id': 'Int64', 'title': 'object'})\n",
    "data = [[1, 'Daniel'], [2, 'Monica'], [3, 'Maria'], [4, 'James']]\n",
    "users = pd.DataFrame(data, columns=['user_id', 'name']).astype({'user_id': 'Int64', 'name': 'object'})\n",
    "data = [[1, 1, 3, '2020-01-12'], [1, 2, 4, '2020-02-11'], [1, 3, 2, '2020-02-12'], [1, 4, 1, '2020-01-01'],\n",
    "        [2, 1, 5, '2020-02-17'], [2, 2, 2, '2020-02-01'], [2, 3, 2, '2020-03-01'], [3, 1, 3, '2020-02-22'],\n",
    "        [3, 2, 4, '2020-02-25']]\n",
    "movie_rating = pd.DataFrame(data, columns=['movie_id', 'user_id', 'rating', 'created_at']).astype(\n",
    "    {'movie_id': 'Int64', 'user_id': 'Int64', 'rating': 'Int64', 'created_at': 'datetime64[ns]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = spark.createDataFrame(movies)\n",
    "users_df = spark.createDataFrame(users)\n",
    "movie_rating_df = spark.createDataFrame(movie_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|movie_id|   title|\n",
      "+--------+--------+\n",
      "|       1|Avengers|\n",
      "|       2|Frozen 2|\n",
      "|       3|   Joker|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|user_id|  name|\n",
      "+-------+------+\n",
      "|      1|Daniel|\n",
      "|      2|Monica|\n",
      "|      3| Maria|\n",
      "|      4| James|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+-------------------+\n",
      "|movie_id|user_id|rating|         created_at|\n",
      "+--------+-------+------+-------------------+\n",
      "|       1|      1|     3|2020-01-12 00:00:00|\n",
      "|       1|      2|     4|2020-02-11 00:00:00|\n",
      "|       1|      3|     2|2020-02-12 00:00:00|\n",
      "|       1|      4|     1|2020-01-01 00:00:00|\n",
      "|       2|      1|     5|2020-02-17 00:00:00|\n",
      "|       2|      2|     2|2020-02-01 00:00:00|\n",
      "|       2|      3|     2|2020-03-01 00:00:00|\n",
      "|       3|      1|     3|2020-02-22 00:00:00|\n",
      "|       3|      2|     4|2020-02-25 00:00:00|\n",
      "+--------+-------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_rating_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the name of the user who has rated the greatest number of movies. \n",
    "In case of a tie, return the lexicographically smaller user name. \n",
    "Find the movie name with the highest average rating in February 2020. \n",
    "In case of a tie, return the lexicographically smaller movie name. \n",
    "The result format is in the following example.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_name = movie_rating_df.groupBy('user_id').agg(F.count('movie_id').alias('movie_count')).join(users_df, on='user_id', how='left').withColumn('name_len', F.length(F.col('name'))).orderBy(['movie_count', 'name_len'], ascending=[False, True]).select('name').limit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Daniel|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_name.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_name = movie_rating_df.filter((F.col('created_at') > '2020-01-31') & (F.col('created_at') < '2020-03-01')).groupBy('movie_id').agg(F.avg(F.col('rating')).alias('avg_rating')).join(movies_df, on='movie_id', how='left').withColumn('title_len', F.length(F.col('title'))).orderBy(['avg_rating', 'title'], ascending=[False, True]).select('title').limit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   title|\n",
      "+--------+\n",
      "|Frozen 2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "title_name.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    name|\n",
      "+--------+\n",
      "|  Daniel|\n",
      "|Frozen 2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df = user_name.unionAll(title_name)\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restaurant Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'Jhon', '2019-01-01', 100], [2, 'Daniel', '2019-01-02', 110], [3, 'Jade', '2019-01-03', 120],\n",
    "        [4, 'Khaled', '2019-01-04', 130], [5, 'Winston', '2019-01-05', 110], [6, 'Elvis', '2019-01-06', 140],\n",
    "        [7, 'Anna', '2019-01-07', 150], [8, 'Maria', '2019-01-08', 80], [9, 'Jaze', '2019-01-09', 110],\n",
    "        [1, 'Jhon', '2019-01-10', 130], [3, 'Jade', '2019-01-10', 150]]\n",
    "       # [1, 'Jhon', '2019-01-10', 130], [3, 'Jade', '2019-01-10', 150],[3, 'Jade', '2019-01-12', 150]]\n",
    "customer = pd.DataFrame(data, columns=['customer_id', 'name', 'visited_on', 'amount']).astype(\n",
    "    {'customer_id': 'Int64', 'name': 'object', 'visited_on': 'datetime64[ns]', 'amount': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = spark.createDataFrame(customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------------+------+\n",
      "|customer_id|   name|         visited_on|amount|\n",
      "+-----------+-------+-------------------+------+\n",
      "|          1|   Jhon|2019-01-01 00:00:00|   100|\n",
      "|          2| Daniel|2019-01-02 00:00:00|   110|\n",
      "|          3|   Jade|2019-01-03 00:00:00|   120|\n",
      "|          4| Khaled|2019-01-04 00:00:00|   130|\n",
      "|          5|Winston|2019-01-05 00:00:00|   110|\n",
      "|          6|  Elvis|2019-01-06 00:00:00|   140|\n",
      "|          7|   Anna|2019-01-07 00:00:00|   150|\n",
      "|          8|  Maria|2019-01-08 00:00:00|    80|\n",
      "|          9|   Jaze|2019-01-09 00:00:00|   110|\n",
      "|          1|   Jhon|2019-01-10 00:00:00|   130|\n",
      "|          3|   Jade|2019-01-10 00:00:00|   150|\n",
      "+-----------+-------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You are the restaurant owner and you want to analyze a possible expansion (there will be at least one customer every day).\n",
    "\n",
    "Compute the moving average of how much the customer paid in a seven days window (i.e., current day + 6 days before). average_amount should be rounded to two decimal places.\n",
    "\n",
    "Return the result table ordered by visited_on in ascending order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_amounts = customer_df.groupBy('visited_on').agg(F.sum('amount').alias('total_amount_daily')).orderBy('visited_on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.orderBy('visited_on').rowsBetween(-6, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+----------+---------+\n",
      "|         visited_on|total_amount_daily|avg_amount|day_count|\n",
      "+-------------------+------------------+----------+---------+\n",
      "|2019-01-07 00:00:00|               150|    122.86|        7|\n",
      "|2019-01-08 00:00:00|                80|     120.0|        7|\n",
      "|2019-01-09 00:00:00|               110|     120.0|        7|\n",
      "|2019-01-10 00:00:00|               280|    142.86|        7|\n",
      "+-------------------+------------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daily_amounts.withColumn('avg_amount', F.round(F.avg('total_amount_daily').over(window_spec), 2)).withColumn('day_count', F.count('total_amount_daily').over(window_spec)).filter(F.col('day_count') == 7).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Friend Requests II: Who Has the Most Friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2, '2016/06/03'], [1, 3, '2016/06/08'], [2, 3, '2016/06/08'], [3, 4, '2016/06/09']]\n",
    "request_accepted = pd.DataFrame(data, columns=['requester_id', 'accepter_id', 'accept_date']).astype(\n",
    "    {'requester_id': 'Int64', 'accepter_id': 'Int64', 'accept_date': 'datetime64[ns]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_accepted_df = spark.createDataFrame(request_accepted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-------------------+\n",
      "|requester_id|accepter_id|        accept_date|\n",
      "+------------+-----------+-------------------+\n",
      "|           1|          2|2016-06-03 00:00:00|\n",
      "|           1|          3|2016-06-08 00:00:00|\n",
      "|           2|          3|2016-06-08 00:00:00|\n",
      "|           3|          4|2016-06-09 00:00:00|\n",
      "+------------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "request_accepted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find the people who have the most friends and the most friends number.\n",
    "\n",
    "The test cases are generated so that only one person has the most friends.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_friends = request_accepted_df.groupBy(F.col('requester_id').alias('id')).agg(F.count_distinct('accepter_id').alias('friends'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|friends|\n",
      "+---+-------+\n",
      "|  1|      2|\n",
      "|  3|      1|\n",
      "|  2|      1|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "req_friends.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept_friends = request_accepted_df.groupBy(F.col('accepter_id').alias('id')).agg(F.count_distinct('requester_id').alias('friends'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|friends|\n",
      "+---+-------+\n",
      "|  3|      2|\n",
      "|  2|      1|\n",
      "|  4|      1|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accept_friends.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|num|\n",
      "+---+---+\n",
      "|  3|  3|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "req_friends.unionAll(accept_friends).groupBy('id').agg(F.sum('friends').alias('num')).orderBy(F.col('num').desc()).limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investments in 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 10, 5, 10, 10], [2, 20, 20, 20, 20], [3, 10, 30, 20, 20], [4, 10, 40, 40, 40]]\n",
    "insurance = pd.DataFrame(data, columns=['pid', 'tiv_2015', 'tiv_2016', 'lat', 'lon']).astype(\n",
    "    {'pid': 'Int64', 'tiv_2015': 'Float64', 'tiv_2016': 'Float64', 'lat': 'Float64', 'lon': 'Float64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance_df = spark.createDataFrame(insurance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+----+----+\n",
      "|pid|tiv_2015|tiv_2016| lat| lon|\n",
      "+---+--------+--------+----+----+\n",
      "|  1|    10.0|     5.0|10.0|10.0|\n",
      "|  2|    20.0|    20.0|20.0|20.0|\n",
      "|  3|    10.0|    30.0|20.0|20.0|\n",
      "|  4|    10.0|    40.0|40.0|40.0|\n",
      "+---+--------+--------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "insurance_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to report the sum of all total investment values in 2016 tiv_2016, for all policyholders who:\n",
    "\n",
    "have the same tiv_2015 value as one or more other policyholders, \n",
    "and are not located in the same city as any other policyholder (i.e., the (lat, lon) attribute pairs must be unique). \n",
    "\n",
    "Round tiv_2016 to two decimal places.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec2 = Window.partitionBy(F.col('location'))\n",
    "window_spec = Window.partitionBy(\"tiv_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+\n",
      "|sum of all total investment values in 2016|\n",
      "+------------------------------------------+\n",
      "|                                      45.0|\n",
      "+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "insurance_df.withColumn('location', F.concat(F.col('lat'), F.col('lon'))).withColumn('location_filter', F.count('pid').over(window_spec2)).filter(F.col('location_filter') == 1).withColumn('tiv_2015_same', F.count('pid').over(window_spec)).filter(F.col('tiv_2015_same') > 1).agg(F.sum('tiv_2016').alias('sum of all total investment values in 2016')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Department Top Three Salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'Joe', 85000, 1], [2, 'Henry', 80000, 2], [3, 'Sam', 60000, 2], [4, 'Max', 90000, 1],\n",
    "        [5, 'Janet', 69000, 1], [6, 'Randy', 85000, 1], [7, 'Will', 70000, 1]]\n",
    "employee = pd.DataFrame(data, columns=['id', 'name', 'salary', 'departmentId']).astype(\n",
    "    {'id': 'Int64', 'name': 'object', 'salary': 'Int64', 'departmentId': 'Int64'})\n",
    "data = [[1, 'IT'], [2, 'Sales']]\n",
    "department = pd.DataFrame(data, columns=['id', 'name']).astype({'id': 'Int64', 'name': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = spark.createDataFrame(employee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------------+\n",
      "| id| name|salary|departmentId|\n",
      "+---+-----+------+------------+\n",
      "|  1|  Joe| 85000|           1|\n",
      "|  2|Henry| 80000|           2|\n",
      "|  3|  Sam| 60000|           2|\n",
      "|  4|  Max| 90000|           1|\n",
      "|  5|Janet| 69000|           1|\n",
      "|  6|Randy| 85000|           1|\n",
      "|  7| Will| 70000|           1|\n",
      "+---+-----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "department_df = spark.createDataFrame(department)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|   IT|\n",
      "|  2|Sales|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A company's executives are interested in seeing who earns the most money in each of the company's departments. A high earner in a department is an employee who has a salary in the top three unique salaries for that department.\n",
    "\n",
    "Write a solution to find the employees who are high earners in each of the departments.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_department = employee_df.join(department_df.withColumnRenamed('name', 'department'), on=employee_df['departmentId'] == department_df['id'], how='left').select([F.col('name').alias('employee'), 'salary', 'department'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----------+\n",
      "|employee|salary|department|\n",
      "+--------+------+----------+\n",
      "|     Joe| 85000|        IT|\n",
      "|     Max| 90000|        IT|\n",
      "|   Janet| 69000|        IT|\n",
      "|   Randy| 85000|        IT|\n",
      "|    Will| 70000|        IT|\n",
      "|   Henry| 80000|     Sales|\n",
      "|     Sam| 60000|     Sales|\n",
      "+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_department.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----------+---------------+\n",
      "|employee|salary|department|department_rank|\n",
      "+--------+------+----------+---------------+\n",
      "|     Max| 90000|        IT|              1|\n",
      "|     Joe| 85000|        IT|              2|\n",
      "|   Randy| 85000|        IT|              2|\n",
      "|    Will| 70000|        IT|              3|\n",
      "|   Henry| 80000|     Sales|              1|\n",
      "|     Sam| 60000|     Sales|              2|\n",
      "+--------+------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(\"department\").orderBy(F.col('salary').desc())\n",
    "employee_department.withColumn('department_rank', F.dense_rank().over(window_spec)).filter(F.col('department_rank') <= 3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
