{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME: C:\\Program Files\\Java\\jdk-17\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"JAVA_HOME:\", os.environ.get(\"JAVA_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"leetcode_tasks\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recyclable and Low Fat Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [['0', 'Y', 'N'], ['1', 'Y', 'Y'], ['2', 'N', 'Y'], ['3', 'Y', 'Y'], ['4', 'N', 'N']]\n",
    "products = pd.DataFrame(data, columns=['product_id', 'low_fats', 'recyclable']).astype(\n",
    "    {'product_id': 'int64', 'low_fats': 'category', 'recyclable': 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- low_fats: string (nullable = true)\n",
      " |-- recyclable: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df = spark.createDataFrame(products)\n",
    "products_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+\n",
      "|product_id|low_fats|recyclable|\n",
      "+----------+--------+----------+\n",
      "|         0|       Y|         N|\n",
      "|         1|       Y|         Y|\n",
      "|         2|       N|         Y|\n",
      "|         3|       Y|         Y|\n",
      "|         4|       N|         N|\n",
      "+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find the ids of products that are both low fat and recyclable.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|product_id|\n",
      "+----------+\n",
      "|         1|\n",
      "|         3|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.filter((products_df['low_fats'] == 'Y') & (products_df['recyclable'] == 'Y')).select('product_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Customer Referee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'Will', None], [2, 'Jane', None], [3, 'Alex', 2], [4, 'Bill', None], [5, 'Zack', 1], [6, 'Mark', 2]]\n",
    "customer = pd.DataFrame(data, columns=['id', 'name', 'referee_id']).astype(\n",
    "    {'id': 'Int64', 'name': 'object', 'referee_id': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = spark.createDataFrame(customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- referee_id: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+\n",
      "| id|name|referee_id|\n",
      "+---+----+----------+\n",
      "|  1|Will|       NaN|\n",
      "|  2|Jane|       NaN|\n",
      "|  3|Alex|       2.0|\n",
      "|  4|Bill|       NaN|\n",
      "|  5|Zack|       1.0|\n",
      "|  6|Mark|       2.0|\n",
      "+---+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the names of the customer that are not referred by the customer with id = 2.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|Will|\n",
      "|Jane|\n",
      "|Bill|\n",
      "|Zack|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.filter(customer_df['referee_id'] != 2).select('name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['Afghanistan', 'Asia', 652230, 25500100, 20343000000], ['Albania', 'Europe', 28748, 2831741, 12960000000],\n",
    "        ['Algeria', 'Africa', 2381741, 37100000, 188681000000], ['Andorra', 'Europe', 468, 78115, 3712000000],\n",
    "        ['Angola', 'Africa', 1246700, 20609294, 100990000000]]\n",
    "world = pd.DataFrame(data, columns=['name', 'continent', 'area', 'population', 'gdp']).astype(\n",
    "    {'name': 'object', 'continent': 'object', 'area': 'Int64', 'population': 'Int64', 'gdp': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_df = spark.createDataFrame(world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------+----------+------------+\n",
      "|       name|continent|   area|population|         gdp|\n",
      "+-----------+---------+-------+----------+------------+\n",
      "|Afghanistan|     Asia| 652230|  25500100| 20343000000|\n",
      "|    Albania|   Europe|  28748|   2831741| 12960000000|\n",
      "|    Algeria|   Africa|2381741|  37100000|188681000000|\n",
      "|    Andorra|   Europe|    468|     78115|  3712000000|\n",
      "|     Angola|   Africa|1246700|  20609294|100990000000|\n",
      "+-----------+---------+-------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "world_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A country is big if:\n",
    "\n",
    "it has an area of at least three million (i.e., 3000000 km2), or it has a population of at least twenty-five million (i.e., 25000000). Write a solution to find the name, population, and area of the big countries.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+\n",
      "|       name|population|   area|\n",
      "+-----------+----------+-------+\n",
      "|Afghanistan|  25500100| 652230|\n",
      "|    Algeria|  37100000|2381741|\n",
      "+-----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "world_df.filter((world_df['area'] >= 3000000) | (world_df['population'] >= 25000000)).select(['name', 'population', 'area']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Views I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 3, 5, '2019-08-01'], [1, 3, 6, '2019-08-02'], [2, 7, 7, '2019-08-01'], [2, 7, 6, '2019-08-02'], [4, 7, 1, '2019-07-22'], [3, 4, 4, '2019-07-21'], [3, 4, 4, '2019-07-21']]\n",
    "views = pd.DataFrame(data, columns=['article_id', 'author_id', 'viewer_id', 'view_date']).astype({'article_id':'Int64', 'author_id':'Int64', 'viewer_id':'Int64', 'view_date':'datetime64[ns]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_df = spark.createDataFrame(views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+-------------------+\n",
      "|article_id|author_id|viewer_id|          view_date|\n",
      "+----------+---------+---------+-------------------+\n",
      "|         1|        3|        5|2019-08-01 00:00:00|\n",
      "|         1|        3|        6|2019-08-02 00:00:00|\n",
      "|         2|        7|        7|2019-08-01 00:00:00|\n",
      "|         2|        7|        6|2019-08-02 00:00:00|\n",
      "|         4|        7|        1|2019-07-22 00:00:00|\n",
      "|         3|        4|        4|2019-07-21 00:00:00|\n",
      "|         3|        4|        4|2019-07-21 00:00:00|\n",
      "+----------+---------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find all the authors that viewed at least one of their own articles.\n",
    "\n",
    "Return the result table sorted by id in ascending order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  4|\n",
      "|  7|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views_df.filter(views_df['author_id'] == views_df['viewer_id']).select(views_df['author_id'].alias('id')).distinct().orderBy('author_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invalid Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'Vote for Biden'], [2, 'Let us make America great again!']]\n",
    "tweets = pd.DataFrame(data, columns=['tweet_id', 'content']).astype({'tweet_id':'Int64', 'content':'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = spark.createDataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|tweet_id|             content|\n",
      "+--------+--------------------+\n",
      "|       1|      Vote for Biden|\n",
      "|       2|Let us make Ameri...|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find the IDs of the invalid tweets. The tweet is invalid if the number of characters used in the content of the tweet is strictly greater than 15.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|tweet_id|\n",
      "+--------+\n",
      "|       2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "tweets_df.filter(F.length(tweets_df['content']) > 15).select('tweet_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Employee ID With The Unique Identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'Alice'], [7, 'Bob'], [11, 'Meir'], [90, 'Winston'], [3, 'Jonathan']]\n",
    "employees = pd.DataFrame(data, columns=['id', 'name']).astype({'id':'int64', 'name':'object'})\n",
    "data = [[3, 1], [11, 2], [90, 3]]\n",
    "employee_uni = pd.DataFrame(data, columns=['id', 'unique_id']).astype({'id':'int64', 'unique_id':'int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|id |name    |\n",
      "+---+--------+\n",
      "|1  |Alice   |\n",
      "|7  |Bob     |\n",
      "|11 |Meir    |\n",
      "|90 |Winston |\n",
      "|3  |Jonathan|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df = spark.createDataFrame(employees)\n",
    "employees_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|id |unique_id|\n",
      "+---+---------+\n",
      "|3  |1        |\n",
      "|11 |2        |\n",
      "|90 |3        |\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_uni_df = spark.createDataFrame(employee_uni)\n",
    "employee_uni_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to show the unique ID of each user, If a user does not have a unique ID replace just show null.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|    name|unique_id|\n",
      "+--------+---------+\n",
      "|     Bob|     NULL|\n",
      "|   Alice|     NULL|\n",
      "|Jonathan|        1|\n",
      "|    Meir|        2|\n",
      "| Winston|        3|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.join(employee_uni_df, on=['id'], how='left').select(['name', 'unique_id']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Sales Analysis I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 100, 2008, 10, 5000], [2, 100, 2009, 12, 5000], [7, 200, 2011, 15, 9000]]\n",
    "sales = pd.DataFrame(data, columns=['sale_id', 'product_id', 'year', 'quantity', 'price']).astype({'sale_id':'Int64', 'product_id':'Int64', 'year':'Int64', 'quantity':'Int64', 'price':'Int64'})\n",
    "data = [[100, 'Nokia'], [200, 'Apple'], [300, 'Samsung']]\n",
    "product = pd.DataFrame(data, columns=['product_id', 'product_name']).astype({'product_id':'Int64', 'product_name':'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+--------+-----+\n",
      "|sale_id|product_id|year|quantity|price|\n",
      "+-------+----------+----+--------+-----+\n",
      "|1      |100       |2008|10      |5000 |\n",
      "|2      |100       |2009|12      |5000 |\n",
      "|7      |200       |2011|15      |9000 |\n",
      "+-------+----------+----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df = spark.createDataFrame(sales)\n",
    "sales_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|product_id|product_name|\n",
      "+----------+------------+\n",
      "|100       |Nokia       |\n",
      "|200       |Apple       |\n",
      "|300       |Samsung     |\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df = spark.createDataFrame(product)\n",
    "product_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to report the product_name, year, and price for each sale_id in the Sales table.\n",
    "\n",
    "Return the resulting table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+\n",
      "|product_name|year|price|\n",
      "+------------+----+-----+\n",
      "|       Nokia|2008| 5000|\n",
      "|       Nokia|2009| 5000|\n",
      "|       Apple|2011| 9000|\n",
      "+------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.join(product_df, on=['product_id'], how='left').select(['product_name', 'year', 'price']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Who Visited but Did Not Make Any Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 23], [2, 9], [4, 30], [5, 54], [6, 96], [7, 54], [8, 54]]\n",
    "visits = pd.DataFrame(data, columns=['visit_id', 'customer_id']).astype({'visit_id':'Int64', 'customer_id':'Int64'})\n",
    "data = [[2, 5, 310], [3, 5, 300], [9, 5, 200], [12, 1, 910], [13, 2, 970]]\n",
    "transactions = pd.DataFrame(data, columns=['transaction_id', 'visit_id', 'amount']).astype({'transaction_id':'Int64', 'visit_id':'Int64', 'amount':'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|visit_id|customer_id|\n",
      "+--------+-----------+\n",
      "|1       |23         |\n",
      "|2       |9          |\n",
      "|4       |30         |\n",
      "|5       |54         |\n",
      "|6       |96         |\n",
      "|7       |54         |\n",
      "|8       |54         |\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visits_df = spark.createDataFrame(visits)\n",
    "visits_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------+\n",
      "|transaction_id|visit_id|amount|\n",
      "+--------------+--------+------+\n",
      "|2             |5       |310   |\n",
      "|3             |5       |300   |\n",
      "|9             |5       |200   |\n",
      "|12            |1       |910   |\n",
      "|13            |2       |970   |\n",
      "+--------------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df = spark.createDataFrame(transactions)\n",
    "transactions_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find the IDs of the users who visited without making any transactions and the number of times they made these types of visits.\n",
    "\n",
    "Return the result table sorted in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|customer_id|count_no_trans|\n",
      "+-----------+--------------+\n",
      "|         54|             2|\n",
      "|         96|             1|\n",
      "|         30|             1|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "visits_df.join(transactions_df, on=['visit_id'], how='left').filter(F.col('transaction_id').isNull()).groupBy('customer_id').agg((F.count('customer_id')).alias('count_no_trans')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rising Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, '2015-01-01', 10], [2, '2015-01-02', 25], [3, '2015-01-03', 20], [4, '2015-01-04', 30]]\n",
    "weather = pd.DataFrame(data, columns=['id', 'recordDate', 'temperature']).astype({'id':'Int64', 'recordDate':'datetime64[ns]', 'temperature':'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------+\n",
      "|id |recordDate         |temperature|\n",
      "+---+-------------------+-----------+\n",
      "|1  |2015-01-01 00:00:00|10         |\n",
      "|2  |2015-01-02 00:00:00|25         |\n",
      "|3  |2015-01-03 00:00:00|20         |\n",
      "|4  |2015-01-04 00:00:00|30         |\n",
      "+---+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df = spark.createDataFrame(weather)\n",
    "weather_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find all dates' Id with higher temperatures compared to its previous dates (yesterday).\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.orderBy(\"recordDate\")\n",
    "weather_with_lag = weather_df.withColumn(\n",
    "    \"prev_temp\",\n",
    "    F.lag(\"temperature\").over(window_spec)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------+---------+\n",
      "| id|         recordDate|temperature|prev_temp|\n",
      "+---+-------------------+-----------+---------+\n",
      "|  1|2015-01-01 00:00:00|         10|     NULL|\n",
      "|  2|2015-01-02 00:00:00|         25|       10|\n",
      "|  3|2015-01-03 00:00:00|         20|       25|\n",
      "|  4|2015-01-04 00:00:00|         30|       20|\n",
      "+---+-------------------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_with_lag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  2|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_with_lag.filter(weather_with_lag['temperature'] > weather_with_lag['prev_temp']).select('id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Time of Process per Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[0, 0, 'start', 0.712], [0, 0, 'end', 1.52], [0, 1, 'start', 3.14], [0, 1, 'end', 4.12], [1, 0, 'start', 0.55],\n",
    "        [1, 0, 'end', 1.55], [1, 1, 'start', 0.43], [1, 1, 'end', 1.42], [2, 0, 'start', 4.1], [2, 0, 'end', 4.512],\n",
    "        [2, 1, 'start', 2.5], [2, 1, 'end', 5]]\n",
    "activity = pd.DataFrame(data, columns=['machine_id', 'process_id', 'activity_type', 'timestamp']).astype(\n",
    "    {'machine_id': 'Int64', 'process_id': 'Int64', 'activity_type': 'object', 'timestamp': 'Float64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------+---------+\n",
      "|machine_id|process_id|activity_type|timestamp|\n",
      "+----------+----------+-------------+---------+\n",
      "|         0|         0|        start|    0.712|\n",
      "|         0|         0|          end|     1.52|\n",
      "|         0|         1|        start|     3.14|\n",
      "|         0|         1|          end|     4.12|\n",
      "|         1|         0|        start|     0.55|\n",
      "|         1|         0|          end|     1.55|\n",
      "|         1|         1|        start|     0.43|\n",
      "|         1|         1|          end|     1.42|\n",
      "|         2|         0|        start|      4.1|\n",
      "|         2|         0|          end|    4.512|\n",
      "|         2|         1|        start|      2.5|\n",
      "|         2|         1|          end|      5.0|\n",
      "+----------+----------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df = spark.createDataFrame(activity)\n",
    "activity_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"There is a factory website that has several machines each running the same number of processes. Write a solution to find the average time each machine takes to complete a process.\n",
    "\n",
    "The time to complete a process is the 'end' timestamp minus the 'start' timestamp. The average time is calculated by the total time to complete every process on the machine divided by the number of processes that were run.\n",
    "\n",
    "The resulting table should have the machine_id along with the average time as processing_time, which should be rounded to 3 decimal places.\n",
    "\n",
    "Return the result table in any order.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = activity_df.groupBy(\"machine_id\", \"process_id\") \\\n",
    "    .pivot(\"activity_type\", [\"start\", \"end\"]) \\\n",
    "    .agg(F.first(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+-----+\n",
      "|machine_id|process_id|start|  end|\n",
      "+----------+----------+-----+-----+\n",
      "|         1|         0| 0.55| 1.55|\n",
      "|         1|         1| 0.43| 1.42|\n",
      "|         0|         1| 3.14| 4.12|\n",
      "|         2|         0|  4.1|4.512|\n",
      "|         0|         0|0.712| 1.52|\n",
      "|         2|         1|  2.5|  5.0|\n",
      "+----------+----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_df = pivot_df.withColumn(\n",
    "    \"processing_time\", \n",
    "    F.col(\"end\") - F.col(\"start\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+-----+------------------+\n",
      "|machine_id|process_id|start|  end|   processing_time|\n",
      "+----------+----------+-----+-----+------------------+\n",
      "|         1|         0| 0.55| 1.55|               1.0|\n",
      "|         1|         1| 0.43| 1.42|              0.99|\n",
      "|         0|         1| 3.14| 4.12|              0.98|\n",
      "|         2|         0|  4.1|4.512|0.4119999999999999|\n",
      "|         0|         0|0.712| 1.52|             0.808|\n",
      "|         2|         1|  2.5|  5.0|               2.5|\n",
      "+----------+----------+-----+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processing_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|machine_id|processing_time_avg|\n",
      "+----------+-------------------+\n",
      "|         0|              0.894|\n",
      "|         1|              0.995|\n",
      "|         2|              1.456|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processing_df.groupBy('machine_id').agg(F.round(F.avg('processing_time'), 3).alias('processing_time_avg')).select(['machine_id', 'processing_time_avg']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employee Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[3, 'Brad', None, 4000], [1, 'John', 3, 1000], [2, 'Dan', 3, 2000], [4, 'Thomas', 3, 4000]]\n",
    "employee = pd.DataFrame(data, columns=['empId', 'name', 'supervisor', 'salary']).astype(\n",
    "    {'empId': 'Int64', 'name': 'object', 'supervisor': 'Int64', 'salary': 'Int64'})\n",
    "data2 = [[2, 500], [4, 2000]]\n",
    "bonus = pd.DataFrame(data2, columns=['empId', 'bonus']).astype({'empId': 'Int64', 'bonus': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema_employee = StructType([\n",
    "    StructField(\"empId\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"supervisor\", IntegerType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "employee_df = spark.createDataFrame(data, schema_employee)\n",
    "bonus_df = spark.createDataFrame(bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+------+\n",
      "|empId|  name|supervisor|salary|\n",
      "+-----+------+----------+------+\n",
      "|    3|  Brad|      NULL|  4000|\n",
      "|    1|  John|         3|  1000|\n",
      "|    2|   Dan|         3|  2000|\n",
      "|    4|Thomas|         3|  4000|\n",
      "+-----+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|empId|bonus|\n",
      "+-----+-----+\n",
      "|    2|  500|\n",
      "|    4| 2000|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bonus_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to report the name and bonus amount of each employee with a bonus less than 1000.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|bonus|\n",
      "+----+-----+\n",
      "|John| NULL|\n",
      "|Brad| NULL|\n",
      "| Dan|  500|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.join(bonus_df, on='empId', how='left').filter((F.col('bonus') < 1000) | (F.col('bonus').isNull())).select(['name', 'bonus']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students and Examinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'Alice'], [2, 'Bob'], [13, 'John'], [6, 'Alex']]\n",
    "students = pd.DataFrame(data, columns=['student_id', 'student_name']).astype(\n",
    "    {'student_id': 'Int64', 'student_name': 'object'})\n",
    "data = [['Math'], ['Physics'], ['Programming']]\n",
    "subjects = pd.DataFrame(data, columns=['subject_name']).astype({'subject_name': 'object'})\n",
    "data = [[1, 'Math'], [1, 'Physics'], [1, 'Programming'], [2, 'Programming'], [1, 'Physics'], [1, 'Math'], [13, 'Math'],\n",
    "        [13, 'Programming'], [13, 'Physics'], [2, 'Math'], [1, 'Math']]\n",
    "examinations = pd.DataFrame(data, columns=['student_id', 'subject_name']).astype(\n",
    "    {'student_id': 'Int64', 'subject_name': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "students_df = spark.createDataFrame(students)\n",
    "subjects_df = spark.createDataFrame(subjects)\n",
    "examinations_df = spark.createDataFrame(examinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Write a solution to find the number of times each student attended each exam.\n",
    "\n",
    "Return the result table ordered by student_id and subject_name.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|student_id|student_name|\n",
      "+----------+------------+\n",
      "|         1|       Alice|\n",
      "|         2|         Bob|\n",
      "|        13|        John|\n",
      "|         6|        Alex|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|subject_name|\n",
      "+------------+\n",
      "|        Math|\n",
      "|     Physics|\n",
      "| Programming|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subjects_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|student_id|subject_name|\n",
      "+----------+------------+\n",
      "|         1|        Math|\n",
      "|         1|     Physics|\n",
      "|         1| Programming|\n",
      "|         2| Programming|\n",
      "|         1|     Physics|\n",
      "|         1|        Math|\n",
      "|        13|        Math|\n",
      "|        13| Programming|\n",
      "|        13|     Physics|\n",
      "|         2|        Math|\n",
      "|         1|        Math|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examinations_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "students_subjects = students_df.join(subjects_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+\n",
      "|student_id|student_name|subject_name|\n",
      "+----------+------------+------------+\n",
      "|         1|       Alice|        Math|\n",
      "|         1|       Alice|     Physics|\n",
      "|         1|       Alice| Programming|\n",
      "|         2|         Bob|        Math|\n",
      "|         2|         Bob|     Physics|\n",
      "|         2|         Bob| Programming|\n",
      "|        13|        John|        Math|\n",
      "|        13|        John|     Physics|\n",
      "|        13|        John| Programming|\n",
      "|         6|        Alex|        Math|\n",
      "|         6|        Alex|     Physics|\n",
      "|         6|        Alex| Programming|\n",
      "+----------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_subjects.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "students_exams = students_subjects.join(examinations_df, on=['student_id', 'subject_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+\n",
      "|student_id|subject_name|student_name|\n",
      "+----------+------------+------------+\n",
      "|        13| Programming|        John|\n",
      "|         2| Programming|         Bob|\n",
      "|         1| Programming|       Alice|\n",
      "|        13|        Math|        John|\n",
      "|         2|        Math|         Bob|\n",
      "|         1|        Math|       Alice|\n",
      "|         1|        Math|       Alice|\n",
      "|         1|        Math|       Alice|\n",
      "|        13|     Physics|        John|\n",
      "|         1|     Physics|       Alice|\n",
      "|         1|     Physics|       Alice|\n",
      "+----------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_exams.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "exams_count = students_exams.withColumn('exam_count',F.when(F.isnull('student_id'),0).otherwise(1)).groupBy(['student_id', 'subject_name', 'student_name']).agg(F.sum('exam_count').alias('attended_exams'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+--------------+\n",
      "|student_id|subject_name|student_name|attended_exams|\n",
      "+----------+------------+------------+--------------+\n",
      "|        13| Programming|        John|             1|\n",
      "|         2| Programming|         Bob|             1|\n",
      "|         1| Programming|       Alice|             1|\n",
      "|        13|        Math|        John|             1|\n",
      "|         2|        Math|         Bob|             1|\n",
      "|         1|        Math|       Alice|             3|\n",
      "|        13|     Physics|        John|             1|\n",
      "|         1|     Physics|       Alice|             2|\n",
      "+----------+------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exams_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+--------------+\n",
      "|student_id|subject_name|student_name|attended_exams|\n",
      "+----------+------------+------------+--------------+\n",
      "|         1|        Math|       Alice|             3|\n",
      "|         1|     Physics|       Alice|             2|\n",
      "|         1| Programming|       Alice|             1|\n",
      "|         2|        Math|         Bob|             1|\n",
      "|         2|     Physics|         Bob|             0|\n",
      "|         2| Programming|         Bob|             1|\n",
      "|         6|        Math|        Alex|             0|\n",
      "|         6|     Physics|        Alex|             0|\n",
      "|         6| Programming|        Alex|             0|\n",
      "|        13|        Math|        John|             1|\n",
      "|        13|     Physics|        John|             1|\n",
      "|        13| Programming|        John|             1|\n",
      "+----------+------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_subjects.join(exams_count, on=['student_id', 'subject_name', 'student_name'], how='left').na.fill(0, 'attended_exams').orderBy(['student_id', 'subject_name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managers with at Least 5 Direct Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[101, 'John', 'A', None], [102, 'Dan', 'A', 101], [103, 'James', 'A', 101], [104, 'Amy', 'A', 101], [105, 'Anne', 'A', 101], [106, 'Ron', 'B', 101]]\n",
    "employee = pd.DataFrame(data, columns=['id', 'name', 'department', 'managerId']).astype({'id':'Int64', 'name':'object', 'department':'object', 'managerId':'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"managerId\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "employee_df = spark.createDataFrame(data,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+---------+\n",
      "| id| name|department|managerId|\n",
      "+---+-----+----------+---------+\n",
      "|101| John|         A|     NULL|\n",
      "|102|  Dan|         A|      101|\n",
      "|103|James|         A|      101|\n",
      "|104|  Amy|         A|      101|\n",
      "|105| Anne|         A|      101|\n",
      "|106|  Ron|         B|      101|\n",
      "+---+-----+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find managers with at least five direct reports.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_count = employee_df.withColumn('report_count',F.when(F.isnull('managerId'),0).otherwise(1)).groupBy(['managerId']).agg(F.sum('report_count').alias('report_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|managerId|report_count|\n",
      "+---------+------------+\n",
      "|      101|           5|\n",
      "|     NULL|           0|\n",
      "+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|John|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.join(report_count, employee_df.id == report_count.managerId, \"left\").filter(F.col('report_count') >= 5).select('name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirmation Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[3, '2020-03-21 10:16:13'], [7, '2020-01-04 13:57:59'], [2, '2020-07-29 23:09:44'], [6, '2020-12-09 10:39:37']]\n",
    "signups = pd.DataFrame(data, columns=['user_id', 'time_stamp']).astype(\n",
    "    {'user_id': 'Int64', 'time_stamp': 'datetime64[ns]'})\n",
    "data = [[3, '2021-01-06 03:30:46', 'timeout'], [3, '2021-07-14 14:00:00', 'timeout'],\n",
    "        [7, '2021-06-12 11:57:29', 'confirmed'], [7, '2021-06-13 12:58:28', 'confirmed'],\n",
    "        [7, '2021-06-14 13:59:27', 'confirmed'], [2, '2021-01-22 00:00:00', 'confirmed'],\n",
    "        [2, '2021-02-28 23:59:59', 'timeout']]\n",
    "confirmations = pd.DataFrame(data, columns=['user_id', 'time_stamp', 'action']).astype(\n",
    "    {'user_id': 'Int64', 'time_stamp': 'datetime64[ns]', 'action': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "signups_df = spark.createDataFrame(signups)\n",
    "confirmations_df = spark.createDataFrame(confirmations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|user_id|         time_stamp|\n",
      "+-------+-------------------+\n",
      "|      3|2020-03-21 10:16:13|\n",
      "|      7|2020-01-04 13:57:59|\n",
      "|      2|2020-07-29 23:09:44|\n",
      "|      6|2020-12-09 10:39:37|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "signups_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------+\n",
      "|user_id|         time_stamp|   action|\n",
      "+-------+-------------------+---------+\n",
      "|      3|2021-01-06 03:30:46|  timeout|\n",
      "|      3|2021-07-14 14:00:00|  timeout|\n",
      "|      7|2021-06-12 11:57:29|confirmed|\n",
      "|      7|2021-06-13 12:58:28|confirmed|\n",
      "|      7|2021-06-14 13:59:27|confirmed|\n",
      "|      2|2021-01-22 00:00:00|confirmed|\n",
      "|      2|2021-02-28 23:59:59|  timeout|\n",
      "+-------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confirmations_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The confirmation rate of a user is the number of 'confirmed' messages divided by the total number of requested confirmation messages. The confirmation rate of a user that did not request any confirmation messages is 0. Round the confirmation rate to two decimal places.\n",
    "\n",
    "Write a solution to find the confirmation rate of each user.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_confirmations = signups_df.join(confirmations_df, on='user_id', how='left').na.fill('timeout', 'action')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+---------+\n",
      "|user_id|         time_stamp|         time_stamp|   action|\n",
      "+-------+-------------------+-------------------+---------+\n",
      "|      7|2020-01-04 13:57:59|2021-06-14 13:59:27|confirmed|\n",
      "|      7|2020-01-04 13:57:59|2021-06-13 12:58:28|confirmed|\n",
      "|      7|2020-01-04 13:57:59|2021-06-12 11:57:29|confirmed|\n",
      "|      6|2020-12-09 10:39:37|               NULL|  timeout|\n",
      "|      3|2020-03-21 10:16:13|2021-07-14 14:00:00|  timeout|\n",
      "|      3|2020-03-21 10:16:13|2021-01-06 03:30:46|  timeout|\n",
      "|      2|2020-07-29 23:09:44|2021-02-28 23:59:59|  timeout|\n",
      "|      2|2020-07-29 23:09:44|2021-01-22 00:00:00|confirmed|\n",
      "+-------+-------------------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_confirmations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_confirmations_count = user_confirmations.groupBy('user_id').pivot('action', ['confirmed', 'timeout']).agg(F.count('action')).na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------+\n",
      "|user_id|confirmed|timeout|\n",
      "+-------+---------+-------+\n",
      "|      7|        3|      0|\n",
      "|      6|        0|      1|\n",
      "|      3|        0|      2|\n",
      "|      2|        1|      1|\n",
      "+-------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_confirmations_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_confirmations_total = user_confirmations_count.withColumn('total', user_confirmations_count['confirmed'] + user_confirmations_count['timeout'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------+-----+\n",
      "|user_id|confirmed|timeout|total|\n",
      "+-------+---------+-------+-----+\n",
      "|      7|        3|      0|    3|\n",
      "|      6|        0|      1|    1|\n",
      "|      3|        0|      2|    2|\n",
      "|      2|        1|      1|    2|\n",
      "+-------+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_confirmations_total.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|user_id|confirmation_rate|\n",
      "+-------+-----------------+\n",
      "|      7|              1.0|\n",
      "|      6|              0.0|\n",
      "|      3|              0.0|\n",
      "|      2|              0.5|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_confirmations_total.withColumn('confirmation_rate', F.round(F.col('confirmed') / F.col('total'), 2)).select(['user_id', 'confirmation_rate']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Aggregate Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not Boring Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'War', 'great 3D', 8.9], [2, 'Science', 'fiction', 8.5], [3, 'irish', 'boring', 6.2],\n",
    "        [4, 'Ice song', 'Fantacy', 8.6], [5, 'House card', 'Interesting', 9.1]]\n",
    "cinema = pd.DataFrame(data, columns=['id', 'movie', 'description', 'rating']).astype(\n",
    "    {'id': 'Int64', 'movie': 'object', 'description': 'object', 'rating': 'Float64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "cinema_df = spark.createDataFrame(cinema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| id|     movie|description|rating|\n",
      "+---+----------+-----------+------+\n",
      "|  1|       War|   great 3D|   8.9|\n",
      "|  2|   Science|    fiction|   8.5|\n",
      "|  3|     irish|     boring|   6.2|\n",
      "|  4|  Ice song|    Fantacy|   8.6|\n",
      "|  5|House card|Interesting|   9.1|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cinema_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to report the movies with an odd-numbered ID and a description that is not \"boring\".\n",
    "\n",
    "Return the result table ordered by rating in descending order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| id|     movie|description|rating|\n",
      "+---+----------+-----------+------+\n",
      "|  5|House card|Interesting|   9.1|\n",
      "|  1|       War|   great 3D|   8.9|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cinema_df.filter((cinema_df['description'] != 'boring') & (F.col('id')%2 != 0)).orderBy('rating', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Selling Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, '2019-02-17', '2019-02-28', 5], [1, '2019-03-01', '2019-03-22', 20], [2, '2019-02-01', '2019-02-20', 15],\n",
    "        [2, '2019-02-21', '2019-03-31', 30]]\n",
    "prices = pd.DataFrame(data, columns=['product_id', 'start_date', 'end_date', 'price']).astype(\n",
    "    {'product_id': 'Int64', 'start_date': 'datetime64[ns]', 'end_date': 'datetime64[ns]', 'price': 'Int64'})\n",
    "data = [[1, '2019-02-25', 100], [1, '2019-03-01', 15], [2, '2019-02-10', 200], [2, '2019-03-22', 30]]\n",
    "units_sold = pd.DataFrame(data, columns=['product_id', 'purchase_date', 'units']).astype(\n",
    "    {'product_id': 'Int64', 'purchase_date': 'datetime64[ns]', 'units': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_df = spark.createDataFrame(prices)\n",
    "units_sold_df = spark.createDataFrame(units_sold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-----+\n",
      "|product_id|         start_date|           end_date|price|\n",
      "+----------+-------------------+-------------------+-----+\n",
      "|         1|2019-02-17 00:00:00|2019-02-28 00:00:00|    5|\n",
      "|         1|2019-03-01 00:00:00|2019-03-22 00:00:00|   20|\n",
      "|         2|2019-02-01 00:00:00|2019-02-20 00:00:00|   15|\n",
      "|         2|2019-02-21 00:00:00|2019-03-31 00:00:00|   30|\n",
      "+----------+-------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prices_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-----+\n",
      "|product_id|      purchase_date|units|\n",
      "+----------+-------------------+-----+\n",
      "|         1|2019-02-25 00:00:00|  100|\n",
      "|         1|2019-03-01 00:00:00|   15|\n",
      "|         2|2019-02-10 00:00:00|  200|\n",
      "|         2|2019-03-22 00:00:00|   30|\n",
      "+----------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "units_sold_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find the average selling price for each product. average_price should be rounded to 2 decimal places.\n",
    "\n",
    "Return the result table in any order\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-----+-------------------+-----+\n",
      "|product_id|         start_date|           end_date|price|      purchase_date|units|\n",
      "+----------+-------------------+-------------------+-----+-------------------+-----+\n",
      "|         1|2019-02-17 00:00:00|2019-02-28 00:00:00|    5|2019-02-25 00:00:00|  100|\n",
      "|         1|2019-02-17 00:00:00|2019-02-28 00:00:00|    5|2019-03-01 00:00:00|   15|\n",
      "|         1|2019-03-01 00:00:00|2019-03-22 00:00:00|   20|2019-02-25 00:00:00|  100|\n",
      "|         1|2019-03-01 00:00:00|2019-03-22 00:00:00|   20|2019-03-01 00:00:00|   15|\n",
      "|         2|2019-02-01 00:00:00|2019-02-20 00:00:00|   15|2019-02-10 00:00:00|  200|\n",
      "|         2|2019-02-01 00:00:00|2019-02-20 00:00:00|   15|2019-03-22 00:00:00|   30|\n",
      "|         2|2019-02-21 00:00:00|2019-03-31 00:00:00|   30|2019-02-10 00:00:00|  200|\n",
      "|         2|2019-02-21 00:00:00|2019-03-31 00:00:00|   30|2019-03-22 00:00:00|   30|\n",
      "+----------+-------------------+-------------------+-----+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prices_df.join(units_sold_df, on='product_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = prices_df.join(units_sold_df, on='product_id').filter((F.col('purchase_date') >= F.col('start_date')) & (F.col('purchase_date') < F.col('end_date')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-----+-------------------+-----+\n",
      "|product_id|         start_date|           end_date|price|      purchase_date|units|\n",
      "+----------+-------------------+-------------------+-----+-------------------+-----+\n",
      "|         1|2019-02-17 00:00:00|2019-02-28 00:00:00|    5|2019-02-25 00:00:00|  100|\n",
      "|         1|2019-03-01 00:00:00|2019-03-22 00:00:00|   20|2019-03-01 00:00:00|   15|\n",
      "|         2|2019-02-01 00:00:00|2019-02-20 00:00:00|   15|2019-02-10 00:00:00|  200|\n",
      "|         2|2019-02-21 00:00:00|2019-03-31 00:00:00|   30|2019-03-22 00:00:00|   30|\n",
      "+----------+-------------------+-------------------+-----+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|product_id|average_price|\n",
      "+----------+-------------+\n",
      "|         1|         6.96|\n",
      "|         2|        16.96|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.groupBy('product_id').agg(F.round(F.sum(F.col('price') * F.col('units')) / F.sum('units'), 2).alias('average_price')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Employees I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 1], [1, 2], [1, 3], [2, 1], [2, 4]]\n",
    "project = pd.DataFrame(data, columns=['project_id', 'employee_id']).astype(\n",
    "    {'project_id': 'Int64', 'employee_id': 'Int64'})\n",
    "data = [[1, 'Khaled', 3], [2, 'Ali', 2], [3, 'John', 1], [4, 'Doe', 2]]\n",
    "employee = pd.DataFrame(data, columns=['employee_id', 'name', 'experience_years']).astype(\n",
    "    {'employee_id': 'Int64', 'name': 'object', 'experience_years': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_df = spark.createDataFrame(project)\n",
    "employee_df = spark.createDataFrame(employee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|project_id|employee_id|\n",
      "+----------+-----------+\n",
      "|         1|          1|\n",
      "|         1|          2|\n",
      "|         1|          3|\n",
      "|         2|          1|\n",
      "|         2|          4|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "project_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----------------+\n",
      "|employee_id|  name|experience_years|\n",
      "+-----------+------+----------------+\n",
      "|          1|Khaled|               3|\n",
      "|          2|   Ali|               2|\n",
      "|          3|  John|               1|\n",
      "|          4|   Doe|               2|\n",
      "+-----------+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Write an SQL query that reports the average experience years of all the employees for each project, rounded to 2 digits.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = project_df.join(employee_df, on='employee_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+----------------+\n",
      "|employee_id|project_id|  name|experience_years|\n",
      "+-----------+----------+------+----------------+\n",
      "|          1|         1|Khaled|               3|\n",
      "|          1|         2|Khaled|               3|\n",
      "|          3|         1|  John|               1|\n",
      "|          2|         1|   Ali|               2|\n",
      "|          4|         2|   Doe|               2|\n",
      "+-----------+----------+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|project_id|average_years|\n",
      "+----------+-------------+\n",
      "|         1|          2.0|\n",
      "|         2|          2.5|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp.groupBy('project_id').agg(F.round(F.avg(F.col('experience_years')), 2).alias('average_years')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage of Users Attended a Contest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[6, 'Alice'], [2, 'Bob'], [7, 'Alex']]\n",
    "users = pd.DataFrame(data, columns=['user_id', 'user_name']).astype({'user_id': 'Int64', 'user_name': 'object'})\n",
    "data = [[215, 6], [209, 2], [208, 2], [210, 6], [208, 6], [209, 7], [209, 6], [215, 7], [208, 7], [210, 2], [207, 2],\n",
    "        [210, 7]]\n",
    "register = pd.DataFrame(data, columns=['contest_id', 'user_id']).astype({'contest_id': 'Int64', 'user_id': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = spark.createDataFrame(users)\n",
    "register_df = spark.createDataFrame(register)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|user_id|user_name|\n",
      "+-------+---------+\n",
      "|      6|    Alice|\n",
      "|      2|      Bob|\n",
      "|      7|     Alex|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|contest_id|user_id|\n",
      "+----------+-------+\n",
      "|       215|      6|\n",
      "|       209|      2|\n",
      "|       208|      2|\n",
      "|       210|      6|\n",
      "|       208|      6|\n",
      "|       209|      7|\n",
      "|       209|      6|\n",
      "|       215|      7|\n",
      "|       208|      7|\n",
      "|       210|      2|\n",
      "|       207|      2|\n",
      "|       210|      7|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "register_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find the percentage of the users registered in each contest rounded to two decimals.\n",
    "\n",
    "Return the result table ordered by percentage in descending order. In case of a tie, order it by contest_id in ascending order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|contest_id|user_id|\n",
      "+----------+-------+\n",
      "|       207|      2|\n",
      "|       208|      2|\n",
      "|       208|      6|\n",
      "|       208|      7|\n",
      "|       209|      2|\n",
      "|       209|      7|\n",
      "|       209|      6|\n",
      "|       210|      6|\n",
      "|       210|      2|\n",
      "|       210|      7|\n",
      "|       215|      6|\n",
      "|       215|      7|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "register_df.orderBy('contest_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|contest_id|user_count|\n",
      "+----------+----------+\n",
      "|       215|         2|\n",
      "|       209|         3|\n",
      "|       208|         3|\n",
      "|       207|         1|\n",
      "|       210|         3|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "register_df.groupBy('contest_id').agg(F.count('user_id').alias('user_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_users = register_df.groupBy('contest_id').agg(F.count('user_id').alias('user_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|contest_id|user_count|\n",
      "+----------+----------+\n",
      "|       215|         2|\n",
      "|       209|         3|\n",
      "|       208|         3|\n",
      "|       207|         1|\n",
      "|       210|         3|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "register_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------+\n",
      "|contest_id|user_count|total_no_users|\n",
      "+----------+----------+--------------+\n",
      "|       215|         2|             3|\n",
      "|       209|         3|             3|\n",
      "|       208|         3|             3|\n",
      "|       207|         1|             3|\n",
      "|       210|         3|             3|\n",
      "+----------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "register_users.withColumn('total_no_users', F.lit(users_df.count())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|contest_id|percentage|\n",
      "+----------+----------+\n",
      "|       209|     100.0|\n",
      "|       208|     100.0|\n",
      "|       210|     100.0|\n",
      "|       215|     66.67|\n",
      "|       207|     33.33|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "register_users.withColumn('total_no_users', F.lit(users_df.count())).withColumn('percentage', F.round((F.col('user_count') / F.col('total_no_users'))*100, 2)).orderBy('percentage', ascending=False).select(['contest_id', 'percentage']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries Quality and Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['Dog', 'Golden Retriever', 1, 5], ['Dog', 'German Shepherd', 2, 5], ['Dog', 'Mule', 200, 1],\n",
    "        ['Cat', 'Shirazi', 5, 2], ['Cat', 'Siamese', 3, 3], ['Cat', 'Sphynx', 7, 4]]\n",
    "queries = pd.DataFrame(data, columns=['query_name', 'result', 'position', 'rating']).astype(\n",
    "    {'query_name': 'object', 'result': 'object', 'position': 'Int64', 'rating': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df = spark.createDataFrame(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------+------+\n",
      "|query_name|          result|position|rating|\n",
      "+----------+----------------+--------+------+\n",
      "|       Dog|Golden Retriever|       1|     5|\n",
      "|       Dog| German Shepherd|       2|     5|\n",
      "|       Dog|            Mule|     200|     1|\n",
      "|       Cat|         Shirazi|       5|     2|\n",
      "|       Cat|         Siamese|       3|     3|\n",
      "|       Cat|          Sphynx|       7|     4|\n",
      "+----------+----------------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Query with rating less than 3 is a poor query.\n",
    "\n",
    "We define query quality as:\n",
    "\n",
    "The average of the ratio between query rating and its position.\n",
    "\n",
    "We also define poor query percentage as:\n",
    "\n",
    "The percentage of all queries with rating less than 3.\n",
    "\n",
    "Write a solution to find each query_name, the quality and poor_query_percentage.\n",
    "\n",
    "Both quality and poor_query_percentage should be rounded to 2 decimal places.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|query_name|quality|\n",
      "+----------+-------+\n",
      "|       Cat|   0.66|\n",
      "|       Dog|    2.5|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries_df.groupBy('query_name').agg(F.round(F.avg(F.col('rating')/F.col('position')), 2).alias('quality')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------------------+\n",
      "|query_name|quality|poor_query_percentage|\n",
      "+----------+-------+---------------------+\n",
      "|       Cat|   0.66|                33.33|\n",
      "|       Dog|    2.5|                33.33|\n",
      "+----------+-------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries_df.groupBy('query_name').agg(F.round(F.avg(F.col('rating')/F.col('position')), 2).alias('quality'), F.round(F.avg(F.when(F.col('rating') < 3, 1).otherwise(0)) * 100, 2).alias('poor_query_percentage')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly Transactions I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[121, 'US', 'approved', 1000, '2018-12-18'], [122, 'US', 'declined', 2000, '2018-12-19'],\n",
    "        [123, 'US', 'approved', 2000, '2019-01-01'], [124, 'DE', 'approved', 2000, '2019-01-07']]\n",
    "transactions = pd.DataFrame(data, columns=['id', 'country', 'state', 'amount', 'trans_date']).astype(\n",
    "    {'id': 'Int64', 'country': 'object', 'state': 'object', 'amount': 'Int64', 'trans_date': 'datetime64[ns]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = spark.createDataFrame(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+------+-------------------+\n",
      "| id|country|   state|amount|         trans_date|\n",
      "+---+-------+--------+------+-------------------+\n",
      "|121|     US|approved|  1000|2018-12-18 00:00:00|\n",
      "|122|     US|declined|  2000|2018-12-19 00:00:00|\n",
      "|123|     US|approved|  2000|2019-01-01 00:00:00|\n",
      "|124|     DE|approved|  2000|2019-01-07 00:00:00|\n",
      "+---+-------+--------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write an SQL query to find for each month and country, the number of transactions and their total amount, the number of approved transactions and their total amount.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_month = transactions_df.withColumn('month', F.substring(F.col('trans_date'), 1, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+------+-------------------+-------+\n",
      "| id|country|   state|amount|         trans_date|  month|\n",
      "+---+-------+--------+------+-------------------+-------+\n",
      "|121|     US|approved|  1000|2018-12-18 00:00:00|2018-12|\n",
      "|122|     US|declined|  2000|2018-12-19 00:00:00|2018-12|\n",
      "|123|     US|approved|  2000|2019-01-01 00:00:00|2019-01|\n",
      "|124|     DE|approved|  2000|2019-01-07 00:00:00|2019-01|\n",
      "+---+-------+--------+------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transaction_month.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+----------------------------------------------+-----------+-------------------------------------------------+\n",
      "|  month|country|count(id)|count(CASE WHEN (state = approved) THEN 1 END)|sum(amount)|sum(CASE WHEN (state = approved) THEN amount END)|\n",
      "+-------+-------+---------+----------------------------------------------+-----------+-------------------------------------------------+\n",
      "|2019-01|     US|        1|                                             1|       2000|                                             2000|\n",
      "|2019-01|     DE|        1|                                             1|       2000|                                             2000|\n",
      "|2018-12|     US|        2|                                             1|       3000|                                             1000|\n",
      "+-------+-------+---------+----------------------------------------------+-----------+-------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transaction_month.groupBy(['month', 'country']).agg(F.count(F.col('id')), F.count(F.when(F.col('state') =='approved', 1)), F.sum(F.col('amount')), F.sum(F.when(F.col('state') =='approved', F.col('amount')))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immediate Food Delivery II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 1, '2019-08-01', '2019-08-02'], [2, 2, '2019-08-02', '2019-08-02'], [3, 1, '2019-08-11', '2019-08-12'],\n",
    "        [4, 3, '2019-08-24', '2019-08-24'], [5, 3, '2019-08-21', '2019-08-22'], [6, 2, '2019-08-11', '2019-08-13'],\n",
    "        [7, 4, '2019-08-09', '2019-08-09']]\n",
    "delivery = pd.DataFrame(data,\n",
    "                        columns=['delivery_id', 'customer_id', 'order_date', 'customer_pref_delivery_date']).astype(\n",
    "    {'delivery_id': 'Int64', 'customer_id': 'Int64', 'order_date': 'datetime64[ns]',\n",
    "     'customer_pref_delivery_date': 'datetime64[ns]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_df = spark.createDataFrame(delivery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-------------------+---------------------------+\n",
      "|delivery_id|customer_id|         order_date|customer_pref_delivery_date|\n",
      "+-----------+-----------+-------------------+---------------------------+\n",
      "|          1|          1|2019-08-01 00:00:00|        2019-08-02 00:00:00|\n",
      "|          2|          2|2019-08-02 00:00:00|        2019-08-02 00:00:00|\n",
      "|          3|          1|2019-08-11 00:00:00|        2019-08-12 00:00:00|\n",
      "|          4|          3|2019-08-24 00:00:00|        2019-08-24 00:00:00|\n",
      "|          5|          3|2019-08-21 00:00:00|        2019-08-22 00:00:00|\n",
      "|          6|          2|2019-08-11 00:00:00|        2019-08-13 00:00:00|\n",
      "|          7|          4|2019-08-09 00:00:00|        2019-08-09 00:00:00|\n",
      "+-----------+-----------+-------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delivery_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If the customer's preferred delivery date is the same as the order date, then the order is called immediate; otherwise, it is called scheduled.\n",
    "\n",
    "The first order of a customer is the order with the earliest order date that the customer made. It is guaranteed that a customer has precisely one first order.\n",
    "\n",
    "Write a solution to find the percentage of immediate orders in the first orders of all customers, rounded to 2 decimal places.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n",
    "delivery_ranked = delivery_df.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "\n",
    "delivery_ranked_filtered = delivery_ranked.withColumn('order_status', F.when(F.col('order_date') == F.col('customer_pref_delivery_date'), 'immediate').otherwise('scheduled')).orderBy(['customer_id', 'order_date']).filter(F.col('row_num') == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-------------------+---------------------------+-------+------------+\n",
      "|delivery_id|customer_id|         order_date|customer_pref_delivery_date|row_num|order_status|\n",
      "+-----------+-----------+-------------------+---------------------------+-------+------------+\n",
      "|          1|          1|2019-08-01 00:00:00|        2019-08-02 00:00:00|      1|   scheduled|\n",
      "|          2|          2|2019-08-02 00:00:00|        2019-08-02 00:00:00|      1|   immediate|\n",
      "|          5|          3|2019-08-21 00:00:00|        2019-08-22 00:00:00|      1|   scheduled|\n",
      "|          7|          4|2019-08-09 00:00:00|        2019-08-09 00:00:00|      1|   immediate|\n",
      "+-----------+-----------+-------------------+---------------------------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delivery_ranked_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+\n",
      "|round((avg(CASE WHEN (order_status = scheduled) THEN 1 ELSE 0 END) * 100), 2)|\n",
      "+-----------------------------------------------------------------------------+\n",
      "|                                                                         50.0|\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delivery_ranked_filtered.agg(F.round(F.avg(F.when(F.col('order_status') == 'scheduled', 1).otherwise(0))*100, 2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Play Analysis IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2, '2016-03-01', 5], [1, 2, '2016-03-02', 6], [2, 3, '2017-06-25', 1], [3, 1, '2016-03-02', 0],\n",
    "        [3, 4, '2018-07-03', 5]]\n",
    "activity = pd.DataFrame(data, columns=['player_id', 'device_id', 'event_date', 'games_played']).astype(\n",
    "    {'player_id': 'Int64', 'device_id': 'Int64', 'event_date': 'datetime64[ns]', 'games_played': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df = spark.createDataFrame(activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+------------+\n",
      "|player_id|device_id|         event_date|games_played|\n",
      "+---------+---------+-------------------+------------+\n",
      "|        1|        2|2016-03-01 00:00:00|           5|\n",
      "|        1|        2|2016-03-02 00:00:00|           6|\n",
      "|        2|        3|2017-06-25 00:00:00|           1|\n",
      "|        3|        1|2016-03-02 00:00:00|           0|\n",
      "|        3|        4|2018-07-03 00:00:00|           5|\n",
      "+---------+---------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to report the fraction of players that logged in again on the day after the day they first logged in, rounded to 2 decimal places. \n",
    "In other words, you need to count the number of players that logged in for at least two consecutive days starting from their first login date, then divide that number by the total number of players.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"player_id\").orderBy(\"event_date\")\n",
    "player_log = activity_df.withColumn(\"row_num\", F.row_number().over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+------------+-------+\n",
      "|player_id|device_id|         event_date|games_played|row_num|\n",
      "+---------+---------+-------------------+------------+-------+\n",
      "|        1|        2|2016-03-01 00:00:00|           5|      1|\n",
      "|        1|        2|2016-03-02 00:00:00|           6|      2|\n",
      "|        2|        3|2017-06-25 00:00:00|           1|      1|\n",
      "|        3|        1|2016-03-02 00:00:00|           0|      1|\n",
      "|        3|        4|2018-07-03 00:00:00|           5|      2|\n",
      "+---------+---------+-------------------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "player_log.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT player_id)|\n",
      "+-------------------------+\n",
      "|                        3|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_players = player_log.agg(F.countDistinct(F.col('player_id'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT player_id)|\n",
      "+-------------------------+\n",
      "|                        2|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loyal_players = player_log.filter(player_log['row_num'] == 2).agg(F.countDistinct(F.col('player_id'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_login_df = activity_df.groupBy(\"player_id\").agg(F.min(\"event_date\").alias(\"first_login_date\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n",
      "|player_id|   first_login_date|\n",
      "+---------+-------------------+\n",
      "|        1|2016-03-01 00:00:00|\n",
      "|        3|2016-03-02 00:00:00|\n",
      "|        2|2017-06-25 00:00:00|\n",
      "+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_login_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = activity_df.join(first_login_df, on=\"player_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+------------+-------------------+\n",
      "|player_id|device_id|         event_date|games_played|   first_login_date|\n",
      "+---------+---------+-------------------+------------+-------------------+\n",
      "|        1|        2|2016-03-01 00:00:00|           5|2016-03-01 00:00:00|\n",
      "|        1|        2|2016-03-02 00:00:00|           6|2016-03-01 00:00:00|\n",
      "|        3|        1|2016-03-02 00:00:00|           0|2016-03-02 00:00:00|\n",
      "|        3|        4|2018-07-03 00:00:00|           5|2016-03-02 00:00:00|\n",
      "|        2|        3|2017-06-25 00:00:00|           1|2017-06-25 00:00:00|\n",
      "+---------+---------+-------------------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "relogin_df = joined_df.withColumn(\"day_after_first_login\", F.date_add(F.col(\"first_login_date\"), 1)).filter(F.col(\"event_date\") == F.col(\"day_after_first_login\")).select(\"player_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_no = joined_df.select('player_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction = round(relogin_df / total_players, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting and Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Unique Subjects Taught by Each Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2, 3], [1, 2, 4], [1, 3, 3], [2, 1, 1], [2, 2, 1], [2, 3, 1], [2, 4, 1]]\n",
    "teacher = pd.DataFrame(data, columns=['teacher_id', 'subject_id', 'dept_id']).astype(\n",
    "    {'teacher_id': 'Int64', 'subject_id': 'Int64', 'dept_id': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_df = spark.createDataFrame(teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+\n",
      "|teacher_id|subject_id|dept_id|\n",
      "+----------+----------+-------+\n",
      "|         1|         2|      3|\n",
      "|         1|         2|      4|\n",
      "|         1|         3|      3|\n",
      "|         2|         1|      1|\n",
      "|         2|         2|      1|\n",
      "|         2|         3|      1|\n",
      "|         2|         4|      1|\n",
      "+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teacher_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to calculate the number of unique subjects each teacher teaches in the university.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|teacher_id|count|\n",
      "+----------+-----+\n",
      "|         1|    2|\n",
      "|         2|    4|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teacher_df.groupBy('teacher_id').agg(F.count_distinct('subject_id').alias('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Activity for the Past 30 Days I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 1, '2019-07-20', 'open_session'], [1, 1, '2019-07-20', 'scroll_down'], [1, 1, '2019-07-20', 'end_session'],\n",
    "        [2, 4, '2019-07-20', 'open_session'], [2, 4, '2019-07-21', 'send_message'], [2, 4, '2019-07-21', 'end_session'],\n",
    "        [3, 2, '2019-07-21', 'open_session'], [3, 2, '2019-07-21', 'send_message'], [3, 2, '2019-07-21', 'end_session'],\n",
    "        [4, 3, '2019-06-25', 'open_session'], [4, 3, '2019-06-25', 'end_session']]\n",
    "activity = pd.DataFrame(data, columns=['user_id', 'session_id', 'activity_date', 'activity_type']).astype(\n",
    "    {'user_id': 'Int64', 'session_id': 'Int64', 'activity_date': 'datetime64[ns]', 'activity_type': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df = spark.createDataFrame(activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------------+-------------+\n",
      "|user_id|session_id|      activity_date|activity_type|\n",
      "+-------+----------+-------------------+-------------+\n",
      "|      1|         1|2019-07-20 00:00:00| open_session|\n",
      "|      1|         1|2019-07-20 00:00:00|  scroll_down|\n",
      "|      1|         1|2019-07-20 00:00:00|  end_session|\n",
      "|      2|         4|2019-07-20 00:00:00| open_session|\n",
      "|      2|         4|2019-07-21 00:00:00| send_message|\n",
      "|      2|         4|2019-07-21 00:00:00|  end_session|\n",
      "|      3|         2|2019-07-21 00:00:00| open_session|\n",
      "|      3|         2|2019-07-21 00:00:00| send_message|\n",
      "|      3|         2|2019-07-21 00:00:00|  end_session|\n",
      "|      4|         3|2019-06-25 00:00:00| open_session|\n",
      "|      4|         3|2019-06-25 00:00:00|  end_session|\n",
      "+-------+----------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find the daily active user count for a period of 30 days ending 2019-07-27 inclusively. A user was active on someday if they made at least one activity on that day.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+\n",
      "|      activity_date|count(DISTINCT user_id)|\n",
      "+-------------------+-----------------------+\n",
      "|2019-07-21 00:00:00|                      2|\n",
      "|2019-07-20 00:00:00|                      2|\n",
      "+-------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df.withColumn('end_date', F.lit('2019-07-27').cast(\"timestamp\")).withColumn('start_date', F.date_add(F.col('end_date'), -30)).filter((F.col('activity_date') >= F.col('start_date')) & (F.col('activity_date') <= F.col('end_date'))).groupBy('activity_date').agg(F.count_distinct('user_id')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Sales Analysis III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 100, 2008, 10, 5000], [2, 100, 2009, 12, 5000], [7, 200, 2011, 15, 9000]]\n",
    "sales = pd.DataFrame(data, columns=['sale_id', 'product_id', 'year', 'quantity', 'price']).astype(\n",
    "    {'sale_id': 'Int64', 'product_id': 'Int64', 'year': 'Int64', 'quantity': 'Int64', 'price': 'Int64'})\n",
    "data = [[100, 'Nokia'], [200, 'Apple'], [300, 'Samsung']]\n",
    "product = pd.DataFrame(data, columns=['product_id', 'product_name']).astype(\n",
    "    {'product_id': 'Int64', 'product_name': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = spark.createDataFrame(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+--------+-----+\n",
      "|sale_id|product_id|year|quantity|price|\n",
      "+-------+----------+----+--------+-----+\n",
      "|      1|       100|2008|      10| 5000|\n",
      "|      2|       100|2009|      12| 5000|\n",
      "|      7|       200|2011|      15| 9000|\n",
      "+-------+----------+----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = spark.createDataFrame(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|product_id|product_name|\n",
      "+----------+------------+\n",
      "|       100|       Nokia|\n",
      "|       200|       Apple|\n",
      "|       300|     Samsung|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to select the product id, year, quantity, and price for the first year of every product sold.\n",
    "\n",
    "Return the resulting table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_first_year = sales_df.groupBy(\"product_id\").agg(F.min(\"year\").alias(\"first_year\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+-----+\n",
      "|product_id|year|quantity|price|\n",
      "+----------+----+--------+-----+\n",
      "|       100|2008|      10| 5000|\n",
      "|       200|2011|      15| 9000|\n",
      "+----------+----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.join(sales_first_year, on='product_id', how='left').filter(F.col('year') == F.col('first_year')).select(['product_id', 'year', 'quantity', 'price']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes More Than 5 Students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['A', 'Math'], ['B', 'English'], ['C', 'Math'], ['D', 'Biology'], ['E', 'Math'], ['F', 'Computer'],\n",
    "        ['G', 'Math'], ['H', 'Math'], ['I', 'Math']]\n",
    "courses = pd.DataFrame(data, columns=['student', 'class']).astype({'student': 'object', 'class': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_df = spark.createDataFrame(courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|student|   class|\n",
      "+-------+--------+\n",
      "|      A|    Math|\n",
      "|      B| English|\n",
      "|      C|    Math|\n",
      "|      D| Biology|\n",
      "|      E|    Math|\n",
      "|      F|Computer|\n",
      "|      G|    Math|\n",
      "|      H|    Math|\n",
      "|      I|    Math|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "courses_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find all the classes that have at least five students.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|class|\n",
      "+-----+\n",
      "| Math|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "courses_df.groupBy('class').agg(F.count_distinct(F.col('student')).alias('number_of_students')).filter(F.col('number_of_students') > 5).select('class').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Followers Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['0', '1'], ['1', '0'], ['2', '0'], ['2', '1']]\n",
    "followers = pd.DataFrame(data, columns=['user_id', 'follower_id']).astype({'user_id': 'Int64', 'follower_id': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_df = spark.createDataFrame(followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|user_id|follower_id|\n",
      "+-------+-----------+\n",
      "|      0|          1|\n",
      "|      1|          0|\n",
      "|      2|          0|\n",
      "|      2|          1|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "followers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution that will, for each user, return the number of followers.\n",
    "\n",
    "Return the result table ordered by user_id in ascending order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|user_id|no_followers|\n",
      "+-------+------------+\n",
      "|      0|           1|\n",
      "|      1|           1|\n",
      "|      2|           2|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "followers_df.groupBy('user_id').agg(F.count_distinct(F.col('follower_id')).alias('no_followers')).orderBy(F.col('no_followers')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biggest Single Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[8], [8], [3], [3], [1], [4], [5], [6]]  # first example\n",
    "#data = [[8], [8], [7], [7], [3], [3], [3]] # second example\n",
    "my_numbers = pd.DataFrame(data, columns=['num']).astype({'num': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_numbers_df = spark.createDataFrame(my_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "|  8|\n",
      "|  8|\n",
      "|  3|\n",
      "|  3|\n",
      "|  1|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_numbers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A single number is a number that appeared only once in the MyNumbers table.\n",
    "\n",
    "Find the largest single number. If there is no single number, report null.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|max|\n",
      "+---+\n",
      "|  6|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_numbers_df.groupBy('num').agg(F.count(F.col('num')).alias('count')).filter(F.col('count') == 1).agg(F.max(F.col('num')).alias('max')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customers Who Bought All Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 5], [2, 6], [3, 5], [3, 6], [1, 6]]\n",
    "customer = pd.DataFrame(data, columns=['customer_id', 'product_key']).astype(\n",
    "    {'customer_id': 'Int64', 'product_key': 'Int64'})\n",
    "data = [[5], [6]]\n",
    "product = pd.DataFrame(data, columns=['product_key']).astype({'product_key': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = spark.createDataFrame(customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|customer_id|product_key|\n",
      "+-----------+-----------+\n",
      "|          1|          5|\n",
      "|          2|          6|\n",
      "|          3|          5|\n",
      "|          3|          6|\n",
      "|          1|          6|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = spark.createDataFrame(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|product_key|\n",
      "+-----------+\n",
      "|          5|\n",
      "|          6|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to report the customer ids from the Customer table that bought all the products in the Product table.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_products = product_df.agg(F.count('product_key')).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|customer_id|no_products_bought|\n",
      "+-----------+------------------+\n",
      "|          1|                 2|\n",
      "|          3|                 2|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.groupBy('customer_id').agg(F.count_distinct(F.col('product_key')).alias('no_products_bought')).filter(F.col('no_products_bought') == number_of_products).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
