{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME: C:\\Program Files\\Java\\jdk-17\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"JAVA_HOME:\", os.environ.get(\"JAVA_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"leetcode_tasks\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recyclable and Low Fat Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [['0', 'Y', 'N'], ['1', 'Y', 'Y'], ['2', 'N', 'Y'], ['3', 'Y', 'Y'], ['4', 'N', 'N']]\n",
    "products = pd.DataFrame(data, columns=['product_id', 'low_fats', 'recyclable']).astype(\n",
    "    {'product_id': 'int64', 'low_fats': 'category', 'recyclable': 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- low_fats: string (nullable = true)\n",
      " |-- recyclable: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df = spark.createDataFrame(products)\n",
    "products_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+\n",
      "|product_id|low_fats|recyclable|\n",
      "+----------+--------+----------+\n",
      "|         0|       Y|         N|\n",
      "|         1|       Y|         Y|\n",
      "|         2|       N|         Y|\n",
      "|         3|       Y|         Y|\n",
      "|         4|       N|         N|\n",
      "+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find the ids of products that are both low fat and recyclable.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|product_id|\n",
      "+----------+\n",
      "|         1|\n",
      "|         3|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.filter((products_df['low_fats'] == 'Y') & (products_df['recyclable'] == 'Y')).select('product_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Customer Referee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'Will', None], [2, 'Jane', None], [3, 'Alex', 2], [4, 'Bill', None], [5, 'Zack', 1], [6, 'Mark', 2]]\n",
    "customer = pd.DataFrame(data, columns=['id', 'name', 'referee_id']).astype(\n",
    "    {'id': 'Int64', 'name': 'object', 'referee_id': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = spark.createDataFrame(customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- referee_id: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+\n",
      "| id|name|referee_id|\n",
      "+---+----+----------+\n",
      "|  1|Will|       NaN|\n",
      "|  2|Jane|       NaN|\n",
      "|  3|Alex|       2.0|\n",
      "|  4|Bill|       NaN|\n",
      "|  5|Zack|       1.0|\n",
      "|  6|Mark|       2.0|\n",
      "+---+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the names of the customer that are not referred by the customer with id = 2.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|Will|\n",
      "|Jane|\n",
      "|Bill|\n",
      "|Zack|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.filter(customer_df['referee_id'] != 2).select('name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['Afghanistan', 'Asia', 652230, 25500100, 20343000000], ['Albania', 'Europe', 28748, 2831741, 12960000000],\n",
    "        ['Algeria', 'Africa', 2381741, 37100000, 188681000000], ['Andorra', 'Europe', 468, 78115, 3712000000],\n",
    "        ['Angola', 'Africa', 1246700, 20609294, 100990000000]]\n",
    "world = pd.DataFrame(data, columns=['name', 'continent', 'area', 'population', 'gdp']).astype(\n",
    "    {'name': 'object', 'continent': 'object', 'area': 'Int64', 'population': 'Int64', 'gdp': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_df = spark.createDataFrame(world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------+----------+------------+\n",
      "|       name|continent|   area|population|         gdp|\n",
      "+-----------+---------+-------+----------+------------+\n",
      "|Afghanistan|     Asia| 652230|  25500100| 20343000000|\n",
      "|    Albania|   Europe|  28748|   2831741| 12960000000|\n",
      "|    Algeria|   Africa|2381741|  37100000|188681000000|\n",
      "|    Andorra|   Europe|    468|     78115|  3712000000|\n",
      "|     Angola|   Africa|1246700|  20609294|100990000000|\n",
      "+-----------+---------+-------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "world_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A country is big if:\n",
    "\n",
    "it has an area of at least three million (i.e., 3000000 km2), or it has a population of at least twenty-five million (i.e., 25000000). Write a solution to find the name, population, and area of the big countries.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+\n",
      "|       name|population|   area|\n",
      "+-----------+----------+-------+\n",
      "|Afghanistan|  25500100| 652230|\n",
      "|    Algeria|  37100000|2381741|\n",
      "+-----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "world_df.filter((world_df['area'] >= 3000000) | (world_df['population'] >= 25000000)).select(['name', 'population', 'area']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Views I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 3, 5, '2019-08-01'], [1, 3, 6, '2019-08-02'], [2, 7, 7, '2019-08-01'], [2, 7, 6, '2019-08-02'], [4, 7, 1, '2019-07-22'], [3, 4, 4, '2019-07-21'], [3, 4, 4, '2019-07-21']]\n",
    "views = pd.DataFrame(data, columns=['article_id', 'author_id', 'viewer_id', 'view_date']).astype({'article_id':'Int64', 'author_id':'Int64', 'viewer_id':'Int64', 'view_date':'datetime64[ns]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_df = spark.createDataFrame(views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+-------------------+\n",
      "|article_id|author_id|viewer_id|          view_date|\n",
      "+----------+---------+---------+-------------------+\n",
      "|         1|        3|        5|2019-08-01 00:00:00|\n",
      "|         1|        3|        6|2019-08-02 00:00:00|\n",
      "|         2|        7|        7|2019-08-01 00:00:00|\n",
      "|         2|        7|        6|2019-08-02 00:00:00|\n",
      "|         4|        7|        1|2019-07-22 00:00:00|\n",
      "|         3|        4|        4|2019-07-21 00:00:00|\n",
      "|         3|        4|        4|2019-07-21 00:00:00|\n",
      "+----------+---------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find all the authors that viewed at least one of their own articles.\n",
    "\n",
    "Return the result table sorted by id in ascending order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  4|\n",
      "|  7|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views_df.filter(views_df['author_id'] == views_df['viewer_id']).select(views_df['author_id'].alias('id')).distinct().orderBy('author_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invalid Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'Vote for Biden'], [2, 'Let us make America great again!']]\n",
    "tweets = pd.DataFrame(data, columns=['tweet_id', 'content']).astype({'tweet_id':'Int64', 'content':'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = spark.createDataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|tweet_id|             content|\n",
      "+--------+--------------------+\n",
      "|       1|      Vote for Biden|\n",
      "|       2|Let us make Ameri...|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find the IDs of the invalid tweets. The tweet is invalid if the number of characters used in the content of the tweet is strictly greater than 15.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|tweet_id|\n",
      "+--------+\n",
      "|       2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "tweets_df.filter(F.length(tweets_df['content']) > 15).select('tweet_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Employee ID With The Unique Identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'Alice'], [7, 'Bob'], [11, 'Meir'], [90, 'Winston'], [3, 'Jonathan']]\n",
    "employees = pd.DataFrame(data, columns=['id', 'name']).astype({'id':'int64', 'name':'object'})\n",
    "data = [[3, 1], [11, 2], [90, 3]]\n",
    "employee_uni = pd.DataFrame(data, columns=['id', 'unique_id']).astype({'id':'int64', 'unique_id':'int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|id |name    |\n",
      "+---+--------+\n",
      "|1  |Alice   |\n",
      "|7  |Bob     |\n",
      "|11 |Meir    |\n",
      "|90 |Winston |\n",
      "|3  |Jonathan|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df = spark.createDataFrame(employees)\n",
    "employees_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|id |unique_id|\n",
      "+---+---------+\n",
      "|3  |1        |\n",
      "|11 |2        |\n",
      "|90 |3        |\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_uni_df = spark.createDataFrame(employee_uni)\n",
    "employee_uni_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to show the unique ID of each user, If a user does not have a unique ID replace just show null.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|    name|unique_id|\n",
      "+--------+---------+\n",
      "|     Bob|     NULL|\n",
      "|   Alice|     NULL|\n",
      "|Jonathan|        1|\n",
      "|    Meir|        2|\n",
      "| Winston|        3|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.join(employee_uni_df, on=['id'], how='left').select(['name', 'unique_id']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Sales Analysis I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 100, 2008, 10, 5000], [2, 100, 2009, 12, 5000], [7, 200, 2011, 15, 9000]]\n",
    "sales = pd.DataFrame(data, columns=['sale_id', 'product_id', 'year', 'quantity', 'price']).astype({'sale_id':'Int64', 'product_id':'Int64', 'year':'Int64', 'quantity':'Int64', 'price':'Int64'})\n",
    "data = [[100, 'Nokia'], [200, 'Apple'], [300, 'Samsung']]\n",
    "product = pd.DataFrame(data, columns=['product_id', 'product_name']).astype({'product_id':'Int64', 'product_name':'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+--------+-----+\n",
      "|sale_id|product_id|year|quantity|price|\n",
      "+-------+----------+----+--------+-----+\n",
      "|1      |100       |2008|10      |5000 |\n",
      "|2      |100       |2009|12      |5000 |\n",
      "|7      |200       |2011|15      |9000 |\n",
      "+-------+----------+----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df = spark.createDataFrame(sales)\n",
    "sales_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|product_id|product_name|\n",
      "+----------+------------+\n",
      "|100       |Nokia       |\n",
      "|200       |Apple       |\n",
      "|300       |Samsung     |\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df = spark.createDataFrame(product)\n",
    "product_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to report the product_name, year, and price for each sale_id in the Sales table.\n",
    "\n",
    "Return the resulting table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+\n",
      "|product_name|year|price|\n",
      "+------------+----+-----+\n",
      "|       Nokia|2008| 5000|\n",
      "|       Nokia|2009| 5000|\n",
      "|       Apple|2011| 9000|\n",
      "+------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.join(product_df, on=['product_id'], how='left').select(['product_name', 'year', 'price']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Who Visited but Did Not Make Any Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 23], [2, 9], [4, 30], [5, 54], [6, 96], [7, 54], [8, 54]]\n",
    "visits = pd.DataFrame(data, columns=['visit_id', 'customer_id']).astype({'visit_id':'Int64', 'customer_id':'Int64'})\n",
    "data = [[2, 5, 310], [3, 5, 300], [9, 5, 200], [12, 1, 910], [13, 2, 970]]\n",
    "transactions = pd.DataFrame(data, columns=['transaction_id', 'visit_id', 'amount']).astype({'transaction_id':'Int64', 'visit_id':'Int64', 'amount':'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|visit_id|customer_id|\n",
      "+--------+-----------+\n",
      "|1       |23         |\n",
      "|2       |9          |\n",
      "|4       |30         |\n",
      "|5       |54         |\n",
      "|6       |96         |\n",
      "|7       |54         |\n",
      "|8       |54         |\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visits_df = spark.createDataFrame(visits)\n",
    "visits_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------+\n",
      "|transaction_id|visit_id|amount|\n",
      "+--------------+--------+------+\n",
      "|2             |5       |310   |\n",
      "|3             |5       |300   |\n",
      "|9             |5       |200   |\n",
      "|12            |1       |910   |\n",
      "|13            |2       |970   |\n",
      "+--------------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df = spark.createDataFrame(transactions)\n",
    "transactions_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find the IDs of the users who visited without making any transactions and the number of times they made these types of visits.\n",
    "\n",
    "Return the result table sorted in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|customer_id|count_no_trans|\n",
      "+-----------+--------------+\n",
      "|         54|             2|\n",
      "|         96|             1|\n",
      "|         30|             1|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "visits_df.join(transactions_df, on=['visit_id'], how='left').filter(F.col('transaction_id').isNull()).groupBy('customer_id').agg((F.count('customer_id')).alias('count_no_trans')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rising Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, '2015-01-01', 10], [2, '2015-01-02', 25], [3, '2015-01-03', 20], [4, '2015-01-04', 30]]\n",
    "weather = pd.DataFrame(data, columns=['id', 'recordDate', 'temperature']).astype({'id':'Int64', 'recordDate':'datetime64[ns]', 'temperature':'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------+\n",
      "|id |recordDate         |temperature|\n",
      "+---+-------------------+-----------+\n",
      "|1  |2015-01-01 00:00:00|10         |\n",
      "|2  |2015-01-02 00:00:00|25         |\n",
      "|3  |2015-01-03 00:00:00|20         |\n",
      "|4  |2015-01-04 00:00:00|30         |\n",
      "+---+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df = spark.createDataFrame(weather)\n",
    "weather_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find all dates' Id with higher temperatures compared to its previous dates (yesterday).\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.orderBy(\"recordDate\")\n",
    "weather_with_lag = weather_df.withColumn(\n",
    "    \"prev_temp\",\n",
    "    F.lag(\"temperature\").over(window_spec)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------+---------+\n",
      "| id|         recordDate|temperature|prev_temp|\n",
      "+---+-------------------+-----------+---------+\n",
      "|  1|2015-01-01 00:00:00|         10|     NULL|\n",
      "|  2|2015-01-02 00:00:00|         25|       10|\n",
      "|  3|2015-01-03 00:00:00|         20|       25|\n",
      "|  4|2015-01-04 00:00:00|         30|       20|\n",
      "+---+-------------------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_with_lag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  2|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_with_lag.filter(weather_with_lag['temperature'] > weather_with_lag['prev_temp']).select('id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Time of Process per Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[0, 0, 'start', 0.712], [0, 0, 'end', 1.52], [0, 1, 'start', 3.14], [0, 1, 'end', 4.12], [1, 0, 'start', 0.55],\n",
    "        [1, 0, 'end', 1.55], [1, 1, 'start', 0.43], [1, 1, 'end', 1.42], [2, 0, 'start', 4.1], [2, 0, 'end', 4.512],\n",
    "        [2, 1, 'start', 2.5], [2, 1, 'end', 5]]\n",
    "activity = pd.DataFrame(data, columns=['machine_id', 'process_id', 'activity_type', 'timestamp']).astype(\n",
    "    {'machine_id': 'Int64', 'process_id': 'Int64', 'activity_type': 'object', 'timestamp': 'Float64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------+---------+\n",
      "|machine_id|process_id|activity_type|timestamp|\n",
      "+----------+----------+-------------+---------+\n",
      "|         0|         0|        start|    0.712|\n",
      "|         0|         0|          end|     1.52|\n",
      "|         0|         1|        start|     3.14|\n",
      "|         0|         1|          end|     4.12|\n",
      "|         1|         0|        start|     0.55|\n",
      "|         1|         0|          end|     1.55|\n",
      "|         1|         1|        start|     0.43|\n",
      "|         1|         1|          end|     1.42|\n",
      "|         2|         0|        start|      4.1|\n",
      "|         2|         0|          end|    4.512|\n",
      "|         2|         1|        start|      2.5|\n",
      "|         2|         1|          end|      5.0|\n",
      "+----------+----------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df = spark.createDataFrame(activity)\n",
    "activity_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"There is a factory website that has several machines each running the same number of processes. Write a solution to find the average time each machine takes to complete a process.\n",
    "\n",
    "The time to complete a process is the 'end' timestamp minus the 'start' timestamp. The average time is calculated by the total time to complete every process on the machine divided by the number of processes that were run.\n",
    "\n",
    "The resulting table should have the machine_id along with the average time as processing_time, which should be rounded to 3 decimal places.\n",
    "\n",
    "Return the result table in any order.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = activity_df.groupBy(\"machine_id\", \"process_id\") \\\n",
    "    .pivot(\"activity_type\", [\"start\", \"end\"]) \\\n",
    "    .agg(F.first(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+-----+\n",
      "|machine_id|process_id|start|  end|\n",
      "+----------+----------+-----+-----+\n",
      "|         1|         0| 0.55| 1.55|\n",
      "|         1|         1| 0.43| 1.42|\n",
      "|         0|         1| 3.14| 4.12|\n",
      "|         2|         0|  4.1|4.512|\n",
      "|         0|         0|0.712| 1.52|\n",
      "|         2|         1|  2.5|  5.0|\n",
      "+----------+----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_df = pivot_df.withColumn(\n",
    "    \"processing_time\", \n",
    "    F.col(\"end\") - F.col(\"start\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+-----+------------------+\n",
      "|machine_id|process_id|start|  end|   processing_time|\n",
      "+----------+----------+-----+-----+------------------+\n",
      "|         1|         0| 0.55| 1.55|               1.0|\n",
      "|         1|         1| 0.43| 1.42|              0.99|\n",
      "|         0|         1| 3.14| 4.12|              0.98|\n",
      "|         2|         0|  4.1|4.512|0.4119999999999999|\n",
      "|         0|         0|0.712| 1.52|             0.808|\n",
      "|         2|         1|  2.5|  5.0|               2.5|\n",
      "+----------+----------+-----+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processing_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|machine_id|processing_time_avg|\n",
      "+----------+-------------------+\n",
      "|         0|              0.894|\n",
      "|         1|              0.995|\n",
      "|         2|              1.456|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processing_df.groupBy('machine_id').agg(F.round(F.avg('processing_time'), 3).alias('processing_time_avg')).select(['machine_id', 'processing_time_avg']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employee Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[3, 'Brad', None, 4000], [1, 'John', 3, 1000], [2, 'Dan', 3, 2000], [4, 'Thomas', 3, 4000]]\n",
    "employee = pd.DataFrame(data, columns=['empId', 'name', 'supervisor', 'salary']).astype(\n",
    "    {'empId': 'Int64', 'name': 'object', 'supervisor': 'Int64', 'salary': 'Int64'})\n",
    "data2 = [[2, 500], [4, 2000]]\n",
    "bonus = pd.DataFrame(data2, columns=['empId', 'bonus']).astype({'empId': 'Int64', 'bonus': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema_employee = StructType([\n",
    "    StructField(\"empId\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"supervisor\", IntegerType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "employee_df = spark.createDataFrame(data, schema_employee)\n",
    "bonus_df = spark.createDataFrame(bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+------+\n",
      "|empId|  name|supervisor|salary|\n",
      "+-----+------+----------+------+\n",
      "|    3|  Brad|      NULL|  4000|\n",
      "|    1|  John|         3|  1000|\n",
      "|    2|   Dan|         3|  2000|\n",
      "|    4|Thomas|         3|  4000|\n",
      "+-----+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|empId|bonus|\n",
      "+-----+-----+\n",
      "|    2|  500|\n",
      "|    4| 2000|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bonus_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to report the name and bonus amount of each employee with a bonus less than 1000.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|bonus|\n",
      "+----+-----+\n",
      "|John| NULL|\n",
      "|Brad| NULL|\n",
      "| Dan|  500|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.join(bonus_df, on='empId', how='left').filter((F.col('bonus') < 1000) | (F.col('bonus').isNull())).select(['name', 'bonus']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students and Examinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'Alice'], [2, 'Bob'], [13, 'John'], [6, 'Alex']]\n",
    "students = pd.DataFrame(data, columns=['student_id', 'student_name']).astype(\n",
    "    {'student_id': 'Int64', 'student_name': 'object'})\n",
    "data = [['Math'], ['Physics'], ['Programming']]\n",
    "subjects = pd.DataFrame(data, columns=['subject_name']).astype({'subject_name': 'object'})\n",
    "data = [[1, 'Math'], [1, 'Physics'], [1, 'Programming'], [2, 'Programming'], [1, 'Physics'], [1, 'Math'], [13, 'Math'],\n",
    "        [13, 'Programming'], [13, 'Physics'], [2, 'Math'], [1, 'Math']]\n",
    "examinations = pd.DataFrame(data, columns=['student_id', 'subject_name']).astype(\n",
    "    {'student_id': 'Int64', 'subject_name': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "students_df = spark.createDataFrame(students)\n",
    "subjects_df = spark.createDataFrame(subjects)\n",
    "examinations_df = spark.createDataFrame(examinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Write a solution to find the number of times each student attended each exam.\n",
    "\n",
    "Return the result table ordered by student_id and subject_name.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|student_id|student_name|\n",
      "+----------+------------+\n",
      "|         1|       Alice|\n",
      "|         2|         Bob|\n",
      "|        13|        John|\n",
      "|         6|        Alex|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|subject_name|\n",
      "+------------+\n",
      "|        Math|\n",
      "|     Physics|\n",
      "| Programming|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subjects_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|student_id|subject_name|\n",
      "+----------+------------+\n",
      "|         1|        Math|\n",
      "|         1|     Physics|\n",
      "|         1| Programming|\n",
      "|         2| Programming|\n",
      "|         1|     Physics|\n",
      "|         1|        Math|\n",
      "|        13|        Math|\n",
      "|        13| Programming|\n",
      "|        13|     Physics|\n",
      "|         2|        Math|\n",
      "|         1|        Math|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examinations_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "students_subjects = students_df.join(subjects_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+\n",
      "|student_id|student_name|subject_name|\n",
      "+----------+------------+------------+\n",
      "|         1|       Alice|        Math|\n",
      "|         1|       Alice|     Physics|\n",
      "|         1|       Alice| Programming|\n",
      "|         2|         Bob|        Math|\n",
      "|         2|         Bob|     Physics|\n",
      "|         2|         Bob| Programming|\n",
      "|        13|        John|        Math|\n",
      "|        13|        John|     Physics|\n",
      "|        13|        John| Programming|\n",
      "|         6|        Alex|        Math|\n",
      "|         6|        Alex|     Physics|\n",
      "|         6|        Alex| Programming|\n",
      "+----------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_subjects.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "students_exams = students_subjects.join(examinations_df, on=['student_id', 'subject_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+\n",
      "|student_id|subject_name|student_name|\n",
      "+----------+------------+------------+\n",
      "|        13| Programming|        John|\n",
      "|         2| Programming|         Bob|\n",
      "|         1| Programming|       Alice|\n",
      "|        13|        Math|        John|\n",
      "|         2|        Math|         Bob|\n",
      "|         1|        Math|       Alice|\n",
      "|         1|        Math|       Alice|\n",
      "|         1|        Math|       Alice|\n",
      "|        13|     Physics|        John|\n",
      "|         1|     Physics|       Alice|\n",
      "|         1|     Physics|       Alice|\n",
      "+----------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_exams.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "exams_count = students_exams.withColumn('exam_count',F.when(F.isnull('student_id'),0).otherwise(1)).groupBy(['student_id', 'subject_name', 'student_name']).agg(F.sum('exam_count').alias('attended_exams'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+--------------+\n",
      "|student_id|subject_name|student_name|attended_exams|\n",
      "+----------+------------+------------+--------------+\n",
      "|        13| Programming|        John|             1|\n",
      "|         2| Programming|         Bob|             1|\n",
      "|         1| Programming|       Alice|             1|\n",
      "|        13|        Math|        John|             1|\n",
      "|         2|        Math|         Bob|             1|\n",
      "|         1|        Math|       Alice|             3|\n",
      "|        13|     Physics|        John|             1|\n",
      "|         1|     Physics|       Alice|             2|\n",
      "+----------+------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exams_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+--------------+\n",
      "|student_id|subject_name|student_name|attended_exams|\n",
      "+----------+------------+------------+--------------+\n",
      "|         1|        Math|       Alice|             3|\n",
      "|         1|     Physics|       Alice|             2|\n",
      "|         1| Programming|       Alice|             1|\n",
      "|         2|        Math|         Bob|             1|\n",
      "|         2|     Physics|         Bob|             0|\n",
      "|         2| Programming|         Bob|             1|\n",
      "|         6|        Math|        Alex|             0|\n",
      "|         6|     Physics|        Alex|             0|\n",
      "|         6| Programming|        Alex|             0|\n",
      "|        13|        Math|        John|             1|\n",
      "|        13|     Physics|        John|             1|\n",
      "|        13| Programming|        John|             1|\n",
      "+----------+------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_subjects.join(exams_count, on=['student_id', 'subject_name', 'student_name'], how='left').na.fill(0, 'attended_exams').orderBy(['student_id', 'subject_name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managers with at Least 5 Direct Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[101, 'John', 'A', None], [102, 'Dan', 'A', 101], [103, 'James', 'A', 101], [104, 'Amy', 'A', 101], [105, 'Anne', 'A', 101], [106, 'Ron', 'B', 101]]\n",
    "employee = pd.DataFrame(data, columns=['id', 'name', 'department', 'managerId']).astype({'id':'Int64', 'name':'object', 'department':'object', 'managerId':'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"managerId\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "employee_df = spark.createDataFrame(data,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+---------+\n",
      "| id| name|department|managerId|\n",
      "+---+-----+----------+---------+\n",
      "|101| John|         A|     NULL|\n",
      "|102|  Dan|         A|      101|\n",
      "|103|James|         A|      101|\n",
      "|104|  Amy|         A|      101|\n",
      "|105| Anne|         A|      101|\n",
      "|106|  Ron|         B|      101|\n",
      "+---+-----+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a solution to find managers with at least five direct reports.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_count = employee_df.withColumn('report_count',F.when(F.isnull('managerId'),0).otherwise(1)).groupBy(['managerId']).agg(F.sum('report_count').alias('report_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|managerId|report_count|\n",
      "+---------+------------+\n",
      "|      101|           5|\n",
      "|     NULL|           0|\n",
      "+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|John|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.join(report_count, employee_df.id == report_count.managerId, \"left\").filter(F.col('report_count') >= 5).select('name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirmation Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[3, '2020-03-21 10:16:13'], [7, '2020-01-04 13:57:59'], [2, '2020-07-29 23:09:44'], [6, '2020-12-09 10:39:37']]\n",
    "signups = pd.DataFrame(data, columns=['user_id', 'time_stamp']).astype(\n",
    "    {'user_id': 'Int64', 'time_stamp': 'datetime64[ns]'})\n",
    "data = [[3, '2021-01-06 03:30:46', 'timeout'], [3, '2021-07-14 14:00:00', 'timeout'],\n",
    "        [7, '2021-06-12 11:57:29', 'confirmed'], [7, '2021-06-13 12:58:28', 'confirmed'],\n",
    "        [7, '2021-06-14 13:59:27', 'confirmed'], [2, '2021-01-22 00:00:00', 'confirmed'],\n",
    "        [2, '2021-02-28 23:59:59', 'timeout']]\n",
    "confirmations = pd.DataFrame(data, columns=['user_id', 'time_stamp', 'action']).astype(\n",
    "    {'user_id': 'Int64', 'time_stamp': 'datetime64[ns]', 'action': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "signups_df = spark.createDataFrame(signups)\n",
    "confirmations_df = spark.createDataFrame(confirmations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|user_id|         time_stamp|\n",
      "+-------+-------------------+\n",
      "|      3|2020-03-21 10:16:13|\n",
      "|      7|2020-01-04 13:57:59|\n",
      "|      2|2020-07-29 23:09:44|\n",
      "|      6|2020-12-09 10:39:37|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "signups_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------+\n",
      "|user_id|         time_stamp|   action|\n",
      "+-------+-------------------+---------+\n",
      "|      3|2021-01-06 03:30:46|  timeout|\n",
      "|      3|2021-07-14 14:00:00|  timeout|\n",
      "|      7|2021-06-12 11:57:29|confirmed|\n",
      "|      7|2021-06-13 12:58:28|confirmed|\n",
      "|      7|2021-06-14 13:59:27|confirmed|\n",
      "|      2|2021-01-22 00:00:00|confirmed|\n",
      "|      2|2021-02-28 23:59:59|  timeout|\n",
      "+-------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confirmations_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The confirmation rate of a user is the number of 'confirmed' messages divided by the total number of requested confirmation messages. The confirmation rate of a user that did not request any confirmation messages is 0. Round the confirmation rate to two decimal places.\n",
    "\n",
    "Write a solution to find the confirmation rate of each user.\n",
    "\n",
    "Return the result table in any order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_confirmations = signups_df.join(confirmations_df, on='user_id', how='left').na.fill('timeout', 'action')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+---------+\n",
      "|user_id|         time_stamp|         time_stamp|   action|\n",
      "+-------+-------------------+-------------------+---------+\n",
      "|      7|2020-01-04 13:57:59|2021-06-14 13:59:27|confirmed|\n",
      "|      7|2020-01-04 13:57:59|2021-06-13 12:58:28|confirmed|\n",
      "|      7|2020-01-04 13:57:59|2021-06-12 11:57:29|confirmed|\n",
      "|      6|2020-12-09 10:39:37|               NULL|  timeout|\n",
      "|      3|2020-03-21 10:16:13|2021-07-14 14:00:00|  timeout|\n",
      "|      3|2020-03-21 10:16:13|2021-01-06 03:30:46|  timeout|\n",
      "|      2|2020-07-29 23:09:44|2021-02-28 23:59:59|  timeout|\n",
      "|      2|2020-07-29 23:09:44|2021-01-22 00:00:00|confirmed|\n",
      "+-------+-------------------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_confirmations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_confirmations_count = user_confirmations.groupBy('user_id').pivot('action', ['confirmed', 'timeout']).agg(F.count('action')).na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------+\n",
      "|user_id|confirmed|timeout|\n",
      "+-------+---------+-------+\n",
      "|      7|        3|      0|\n",
      "|      6|        0|      1|\n",
      "|      3|        0|      2|\n",
      "|      2|        1|      1|\n",
      "+-------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_confirmations_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_confirmations_total = user_confirmations_count.withColumn('total', user_confirmations_count['confirmed'] + user_confirmations_count['timeout'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------+-----+\n",
      "|user_id|confirmed|timeout|total|\n",
      "+-------+---------+-------+-----+\n",
      "|      7|        3|      0|    3|\n",
      "|      6|        0|      1|    1|\n",
      "|      3|        0|      2|    2|\n",
      "|      2|        1|      1|    2|\n",
      "+-------+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_confirmations_total.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|user_id|confirmation_rate|\n",
      "+-------+-----------------+\n",
      "|      7|              1.0|\n",
      "|      6|              0.0|\n",
      "|      3|              0.0|\n",
      "|      2|              0.5|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_confirmations_total.withColumn('confirmation_rate', F.round(F.col('confirmed') / F.col('total'), 2)).select(['user_id', 'confirmation_rate']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
